#+SETUPFILE: ./theme-bigblow-local.setup
#+TITLE: Parallele Algorithmen
#+SUBTITLE: Parallelprogrammierung
# Titel fraglich
#+AUTHOR: Johannes Brauer
#+OPTIONS:   H:4
#+OPTIONS: num:nil d:true
#+OPTIONS: toc:nil
#+OPTIONS: reveal_single_file:nil
#+Language:  de
#+STARTUP: latexpreview
#+STARTUP: inlineimages
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="mycss/mystyle.css" />
# +REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/3.0.0/
#+REVEAL_THEME: simple
#+REVEAL_TRANS: slide
#+REVEAL_HLEVEL: 1
#+REVEAL_INIT_SCRIPT: dependencies: [ { src: 'plugin/menu/menu.js', async: true },
#+REVEAL_INIT_SCRIPT:                 { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true } ]
#+REVEAL_MARGIN: 0.05
#+REVEAL_EXTRA_CSS: ./mycss/myrevealstyle.css
#+OPTIONS: reveal_control:t
# um Folien mit reveal.js erzeugen zu können:ml
# M-x load-library und dann ox-reveal

* Grundlagen 
# Chapter 10
** Overview of Parallelism Beyond Concurrency
# Chapter 10 parallelismOverview.txt
 Parallelism in computing

+ So far the emphasis has been on parallelism as concurrency made
  efficient: multiple concurrent tasks are distributed among
  processors so that the tasks can be processed literally at the same
  time--in parallel.
+ In a multiprocessing approach to concurrency, multiple processes
  execute on multiple processors.
  + The OS handles the scheduling.
+ In a multithreading approach to concurrency, multiple threads within a process execute on multiple processors.
  + If API threads map to /native/ threads, then the OS handles the scheduling. (Assuming no GIL.)
+ In hybrid approaches (e.g., Asp.Net's multithreaded worker
  processes), there's still the distribution of task processing among
  multiple processors.

The goal now is to focus on broader uses of parallel computation.

+ Some reasons why parallelism is now so attractive:
  + Consumer desktop, laptop, and even handheld machines are now
    multi-core, with one or more processors per core.
    + In fact (even if not in reputation), these machines are /parallel computers/.
    + At issue is how to exploit this possibily for parallel computations.
  + Consumer-level machines may come with GPUs (graphics-processing
    units) and FPUs (floating-point unit or /math coproccessor/) as
    well as CPUs (central or /generic/ processing units).
    + GPUs and FPUs are designed for parallelism.
    + /gpgpu/ (general-purpose computing on a GPU) refers to the use
      of a GPU for /general purposes/, that is, purposes beyond just
      graphics. So gpgpu approaches a GPU as if it were a
      (specialized) CPU.
  + Distributed software systems such as email and the WWW have code
    modules that execute on physically distinct devices.
    + The /network as the computer/ describes networked hosts, each
      with its own collection of processors, as a resource for
      distributed parallel computation.
    + The key challenge is how to coordinate such computations and to share information among them.
      + Message-passing is a popular, general method.
+ Parallel computations may used shared, centralized memory;
  distributed memory; or some hybrid of the two.
  + The point of interest is how to write programs that execute as
    parallel computations.
  + Support from toolchains (in particular, compilers therein) is
    critical to encouarge widespread use of hardware support for
    parallel computing.

** A SIMD Auto Vectorization Code Example in C
*** the non parallel version
#+BEGIN_SRC C
/** To compile and run:

    gcc -o nonsimd nonsimd.c
    ./nonsimd  # On Windows: simd
,*/
  
#include <stdio.h>

#define Length 8

int main() {
  int array1[ ] = {1, 2, 3, 4, 5, 6, 7, 8}; 
  int array2[ ] = {8, 7, 6, 5, 4, 3, 2, 1}; 
  int sum[Length];

  int i;
  for (i = 0; i < Length; i++) sum[i] = array1[i] + array2[i]; /** serial computation **/
  
  for (i = 0; i < Length; i++) printf("%i ", sum[i]);  /** ideal case for data parallelism **/
  putchar('\n');
  return 0;
}
/** A better way: SSE = Streaming SIMD Extensions = additional instruction set, 
    more CPU registers

In the late 1990s, Intel introduced an SSE instruction set for their Pentium III
processors. This became known as the MMX instruction set. (Officially, MMX is not an
acronym.) AMD earlier introduced a comparable instruction set known as 3DNow!. In any case, 
the hardware support included new registers with names that start with 'xmm'. In the 
original SSE, there was support only for integer operations.

SEE has progressed, with new versions: SSE2, SSE3, SSSE3, and
SSE4. The newer versions bring SIMD support for both integer and
floating-point operations. At the same time, C/C++ compilers such as
GNU and Clang provided support, eventually automatic, for
SIMD-friendly code. Nowadays, both compilers are capable of 'automatic
vectorization' of approrpiately written source code.

The short version: elements in the arrays 'array1' and 'array2' should
be added pairwise in parallel in order to boost performance.
,*/
#+END_SRC
*** the parallel version
#+BEGIN_SRC C
/** To compile and run:

    gcc -o simd simd.c
    ./simd  # On Windows: simd

 No explicit optimization is required. The GNU and Clang compilers
 do the optimization automatically. 
*/
  
#include <stdio.h>

/* A typedef in C supports 'aliasing' of data types. Here's a simple example:

     typedef unsigned boolean; ## 'unsigned' is short for 'unsigned int'

   This typedef allows 'boolean' to be used in place of 'unsigned', for example:

     boolean flag = 0;  ## 0 is 'false' in C

   The typedef below is more complicated, as it includes special information for the
   compiler, in particuar the '__attribute__' and 'vector_size' terms. Here's the upshot
   of the typedef:

   # 'intV8' aliases 'int', but as a vector (1-dimensional array) rather than as a scalar 
      type.
   # The size of an 'intV8' vector is set to 32 == Length * sizeof(int) == 8 * 4. 
     The size is in bytes.
*/
#define Length 8
typedef int intV8 __attribute__ ((vector_size (Length * sizeof(int)))); /** 32 bytes **/
    
int main() {
  intV8 dataV1 = {1, 2, 3, 4, 5, 6, 7, 8}; /* initialize vector dataV1: no array syntax */                   
  intV8 dataV2 = {8, 7, 6, 5, 4, 3, 2, 1}; /* same for vector dataV2 */
  
  intV8 add = dataV1 + dataV2; /* 9  9  9  9  9  9  9 9 -- no array syntax */
  intV8 mul = dataV1 * dataV2; /* 8 14 18 20 20 18 14 8 -- no array syntax */
  intV8 div = dataV2 / dataV1; /* 8  3  2  1  0  0  0 0 -- no array syntax */
  int i;
  for (i = 0; i < Length; i++) printf("%i ", add[i]); /* array syntax */
  putchar('\n');
  return 0;
}
#+END_SRC
** using the shell
#+BEGIN_SRC sh 
bash-3.2$ gcc -o nonsimd nonsimd.c
bash-3.2$ gcc -o simd simd.c

bash-3.2$ ./nonsimd
9 9 9 9 9 9 9 9 

bash-3.2$ ./simd
9 9 9 9 9 9 9 9 
#+END_SRC
** in Assembly-Code
# chapter 10 simdDoc.s
# ev. auslassen
 Some compiler-generated assembly code from the simd.c program
#+BEGIN_SRC
# A note on C compilation:

                 +------------+    +--------+    +---------+    +------+
C source code--->|preprocessor|--->|compiler|--->|assembler|--->|linker|--->executable 	
                 +------------+    +--------+ /  +---------+ /  +------+    machine code
                                             /              /
                                        assembly code    object code

## Compile command: gcc -S simd.c  (produces 'simd.s' by default)	
	
main:
   ## The identifiers that start with $ are constants.
	
   ## %rsp is the /stack pointer/, which points to the TOP position on the stack.
	
   ## An expression such as 96(%rsp) is an address or pointer expression: %rsp is the base 
      address, 96 is of the offset therefrom. The offset is in bytes, so in this case it/s 
      96 bytes. (The Intel stack grows downwards, from high to low addresses.)

   ## The full expression 'movl $1, 96(%rsp)' copies (not literally moves) the 32-bit value 
      1 into stack location %rsp + 96:

	The stack
	  ...                     ## high addresses
	+-----+
	|  1  |<------%rsp + 96   ## instruction 'movl $l, 96(%rsp)' puts 1 in this location
        +-----+
	  ...                
	+-----+
	|     |
	+-----+
	|     |<------%rsp   ## %rsp is the 64-bit 'stack pointer'
	+-----+
	  ...                     ## low addresses
	
   ## Note that every offset increases by 4 because the values (1,2,3,...) are 4-byte 
      (32-bit) integer values.
   movl $1, 96(%rsp)         # data vector 1 start
   movl $2, 100(%rsp)
   movl $3, 104(%rsp)
   movl $4, 108(%rsp)
   movl $5, 112(%rsp)
   movl $6, 116(%rsp)
   movl $7, 120(%rsp)
   movl $8, 124(%rsp)        # data vector 1 end
   movl $8, 128(%rsp)        # data vector 2 start
   movl $7, 132(%rsp)
   movl $6, 136(%rsp)
   movl $5, 140(%rsp)
   movl $4, 144(%rsp)
   movl $3, 148(%rsp)
   movl $2, 152(%rsp)
   movl $1, 156(%rsp)        # data vector 2 end
	
   movdqa 96(%rsp), %xmm1    # %xmm1 = dataV1[0..3]: 'dq' is for 'double quadword'
	                     # (quad == 64 bits) and %xmm1 is a 128-bit register
	
   movdqa 128(%rsp), %xmm0   # %xmm0 = dataV2[0..3]
	
   paddd %xmm0, %xmm1        # %xmm1 += %xmm0 (the 'paddd' instruction is for 'packed 
                               addition' of 32-bit values)
   movdqa 112(%rsp), %xmm2   # %xmm2 = dataV1[4..7]
   movdqa 144(%rsp), %xmm0   # %xmm0 = dataV2[4..7]
   paddd %xmm2, %xmm0        # %xmm0 += %xmm2
	
   movdqa %xmm1, 32(%rsp)    # save %xmm1 to stack (sums of the 1st four elements)
   movdqa %xmm0, 48(%rsp)    # save %xmm0 to stack (sums of the last four elements)
	                     # the sum is now on the stack contiguously, in effect as an array
   ...

#+END_SRC
* Zeitmessungen -- ein Code-Beispiel
# chapter 10
** Source Code
# ssETimings.c
#+BEGIN_SRC C
/** A timing example to contrast serial and parallel execution on typical matrix/vector 
    operations. The program uses 'SSE intrinsics', the interface for which is the header 
    file 'xmmintrin.h'.
    
    Some of the code is obscure, a good sign of what's involved as you get ever closer 
    to the metal.

    The program adds two vectors, and then multiples the rows of the sum vector by a 
    +third vector.
**/

#include <stdio.h>
#include <xmmintrin.h>
#include <stdint.h>

typedef void (*func)();  /** a function that takes no arguments and returns void **/

#define Length      (2048)
#define AlignedVals    (4)

 /** Ensure 16-byte alignment of vector elements: 16 bytes is 4 ints or 4 floats,
     which comes to 128 bits--the size of an %xmm register. **/
float v1[Length] __attribute__ ((aligned(16))); 
float v2[Length] __attribute__ ((aligned(16)));
float v3[Length] __attribute__ ((aligned(16)));
float r1[Length] __attribute__ ((aligned(16)));
float r2[Length] __attribute__ ((aligned(16)));

/** Initialize with small random values. **/
void init() {
  unsigned i;
  for (i = 0; i < Length; i++) {
    v1[i] = (float) (rand() / RAND_MAX); 
    v2[i] = (float) (rand() / RAND_MAX);  
    v3[i] = (float) (rand() / RAND_MAX); 
  }
}

void in_serial() {
  unsigned i;
  for (i = 0; i < Length; i++) r1[i] = (v1[i] + v2[i]) * v3[i]; /** the math **/
}

void in_parallel() {
  /** set up for the special vector instructions **/
  __m128* mm_v1 = (__m128*) v1; 
  __m128* mm_v2 = (__m128*) v2;
  __m128* mm_v3 = (__m128*) v3;
  __m128* mm_rv = (__m128*) r2;

  /** Multiply a vector row by a vector: the outer instruction is _mm_mul_ps. 
      _mm_add_ps adds vector elements.
   **/
  unsigned i;
  unsigned n = Length / AlignedVals; /** in this case, n == 512 **/
  for( i = 0; i < n; i++) mm_rv[i] = _mm_mul_ps(_mm_add_ps(mm_v1[i], mm_v2[i]), mm_v3[i]);
}

/** from the Intel developer's guide: in-line assembly for reasonably good timings **/
uint64_t rdtsc() {
    uint32_t lo, hi;
    __asm__ __volatile__ (
      "xorl %%eax, %%eax\n"
      "cpuid\n"
      "rdtsc\n"
      : "=a" (lo), "=d" (hi)
      :
      : "%ebx", "%ecx");
    return (uint64_t) hi << 32 | lo;
}

void time_it(const char* msg, func func2run) {
  unsigned long long start, stop;
  start = rdtsc();
  func2run();
  stop = rdtsc();
  printf("%s: %lld\n", msg, stop - start);
}

int main() {
  init();
  time_it("in_serial()  ", in_serial);
  time_it("in_parallel()", in_parallel);
  return 0;
}
#+END_SRC
** using the shell
#+BEGIN_SRC sh
bash-3.2$ gcc -o sseTimings sseTimings.c
bash-3.2$ ./sseTimings
in_serial()  : 19905
in_parallel(): 10483
bash-3.2$ ./sseTimings
in_serial()  : 22260
in_parallel(): 11319
bash-3.2$ ./sseTimings
in_serial()  : 21785
in_parallel(): 10957
#+END_SRC
# * OpenMp
# # noch Chapter 10
# # entweder nachtragen oder an Studi vergeben
# ** Overview
# ** Basics
# ** Miser-Spendthrift Problem
* Parallelität und Performance
# chapter 11
** How Can Performance Gains Through Parallelism Be Measured
 Parallelism and performance

+ Revised /sseTimings/ program, now in 3 versions: /timings2048/,
  /timings8192/, and /timings32768/:
  + Average speedup of 'serial' version from 20 test runs:\\
       'timings2048':  average speedup of 2.17    ## 2048 is vector size\\
       'timings8192':  average speedup of 1.72    ##  8192 == 2048 x 4\\
       'timings32768': average speedup of 1.65    ## 32768 == 8192 x 4
  + Key question: The 'in_parallel()' function does only 1/4th the
    arithmetic operations as the 'in_serial()' function. Why is max
    speedup above only slightly over 2 rather than close to (or even
    at) 4?
------------------------------------------------------------------------------------------------------------   
+ How to measure performance?
  + Response time ('latency'): elapsed time
    + Program P1 takes 10 units, whereas program P2 takes 5: P1/P2 =
      2, a speedup of 2
  + Throughput: number of tasks completed in a fixed amount of time.
    + On single-processor machine M1, with required times for processing above the arrows:
      #+begin_example
    5       3   +-----------+
T2----->T1----->| processor |  ## Two tasks completed in 8 time units on M1
                +-----------+
      #+end_example
    + On multiple-processor machine M2:
      #+begin_example
   5   +-----------+
T1----->| processor | ## T1 takes a bit longer on M2
       +-----------+
               ## Two tasks completed in 5 time units on M2, a speedup of about 1.6
   5   +-----------+
T2----->| processor | ## T2 takes the same amount of time on M1 and M2
       +-----------+
      #+end_example
  + Big picture: parallelism is a good way to boost performance
    + Parallelism tends to increase throughput, and thereby performance.

** The ParallelSort Program in Java
#+BEGIN_SRC java
/** A Java program to illustrate the potential speedup of parallelism, in this
    case for sorting: the program uses the new (as of Java 8) Arrays.parallelSort
    method, and also the Arrays.parallelSetAll method for parallel initialization
    of the roughly 4M array of integers.

    As noted, Java 8 or greater is requied.
*/
import java.util.Random;
import java.util.Arrays;

public class ParallelSort {
    private int[ ] array2Sort;
    
    public static void main(String[ ] args) {
	new ParallelSort().doIt();
    }
    private void doIt() {
	final int arraySize = 1024 * 1024 * 4;  // about 4M: 4,194,304     /** line 1 **/

	//** serial sort
        init(arraySize);
	long serial = sortArray(array2Sort, false);                        /** line 2 **/

	//** parallel sort
	init(arraySize);
	long parallel = sortArray(array2Sort, true);                       /** line 3 **/

	//** report
	log("Serial sort of array of size:   " + arraySize);
	log("Elapsed time: " + serial);
	log("");

	log("Parallel sort of array of size: " + arraySize); 
	log("Elapsed time: " + parallel);

	log("");
	double ratio = (double) serial / (double) parallel;
	if (parallel < serial) 
	    log("Speedup:  " + ratio);
	else
	    log("Slowdown: " + ratio);
    }
    private void init(int size) {
	array2Sort = new int[size];
	/** parallel initialization of array elements **/
	Arrays.parallelSetAll(array2Sort,                                  /** line 4 **/ 
			      i->new Random().nextInt());                  /** line 5 **/
    }
    private long sortArray(int[ ] array, boolean parallel) {
	long start = System.currentTimeMillis();
	/** parallel version of merge-sort **/
	if (parallel) Arrays.parallelSort(array);                          /** line 6 **/
	/** variant of quicksort ('dual pivot') **/
	else Arrays.sort(array);                                           /** line 7 **/
	long stop = System.currentTimeMillis();
	return stop - start;
    }
    private void log(String msg) {
	System.out.println(msg);
    }
}
#+END_SRC
*** using the shell
#+BEGIN_SRC sh
bash-3.2$ java ParallelSort
Serial sort of array of size:   4194304
Elapsed time: 824

Parallel sort of array of size: 4194304
Elapsed time: 358

Speedup:  2.3016759776536313
bash-3.2$ java ParallelSort
Serial sort of array of size:   4194304
Elapsed time: 710

Parallel sort of array of size: 4194304
Elapsed time: 258

Speedup:  2.751937984496124
bash-3.2$ java ParallelSort
Serial sort of array of size:   4194304
Elapsed time: 807

Parallel sort of array of size: 4194304
Elapsed time: 295

Speedup:  2.7355932203389832
#+END_SRC
** Amdahls Law by Example

+ The law, named after its creator (a one-time IBM engineer who
  founded a IBM rival company), sets a theoretical limit on the
  speedup that can be achieved in a computation.
  + The law has the practical benefit of sharpening intuitions about
    how speedup should be pursued.
+ A sample problem for Amdahl's law: 
  + Compute the arithmetic mean of 4M floating-point values on
    single-processor machine M1.  Call this problem P.
  + Compute the speedup of porting the solution to quad-processor
    machine M4.
    + Assume that M1 and M4 differ only in the number of processors.
+ A step-by-step application of the law:
  1. Compute response time for P on M1: $RT1$
  2. Compute response time for P on M4: $RT4$
  3. Computer the ratio: $\frac{RT1}{RT4}$
     + If $\frac{RT1}{RT4} = 1$, the response times are the same: no difference between M1 and M4 performance.
     + If $\frac{RT1}{RT4} < 1$, there's slowdown rather than speedup.
     + If $\frac{RT1}{RT4} > 1$, there's speedup.
  4. Normalize $RT1$ to 1 time unit: keeps things simple.
  5. Break $RT4$ into two parts: 
     + $enchanced$: in this case, parallel and
     + $unenhanced$: in this case, serial
     + The $unenhanced$ part of $RT4$ is the part still done in serial
       mode -- no benefit of parallelism.
       + The data set needs to be partitioned, assigned to workers
         (threads or processes), the results from the workers need to
         be aggregated, etc.
     + The $enhanced$ part of $RT4$ is the part that benefits from the
       speedup due to parallelism.
     + $$Speedup = \frac{RT1}{RT4} = \frac{1}{unenhanced + enhanced}$$
     + The $enhanced$ depends on two factors: what fraction of time is
       sped up, and by what amount.  How much time does M4 speed in
       'parallel mode' when solving P, and how much faster is M4 in
       'parallel mode' than M1 in serial mode?
     + Assume M4 is four times faster than M1 when M4 is in 'parallel
       mode', and that M4 is in 'parallel mode' 80% of the time when
       solving P.
     + $$Speedup = \frac{RT1}{RT4} = \frac{1}{unenhanced + enhanced} = \frac{1}{0.2 + \frac{0.8}{4.0}} = 2.5$$
     + Under the given assumptions, M4 solves P 2.5 times faster than does M1.
+ The big picture of Amdahal's Law:
  + It's tempting to be distracted by how much faster a 'local
    speedup' is: M4 is four times faster than M1 when M4 is in
    'parallel mode'.
  + It's critical to focus on the fraction of time spent in enhanced
    mode: M4 is in 'parallel mode' 80% of the time--not 100% of time.

| Machine | Fraction in local speedup | Local speedup | Global speedup |
|---------+---------------------------+---------------+----------------|
| M4      |                       0.8 |           4.0 |            2.5 |
| M8      |                       0.7 |           8.0 |            2.6 |
| M12     |                       0.6 |          12.0 |          *2.2* |
| M16     |                       0.5 |          16.0 |            1.9 |
| M20     |                       0.4 |          20.0 |            1.6 |
| M24     |                       0.3 |          24.0 |            1.4 |

** Programming exercise for parallelism/performance
+ More data gathering and reflecting on the data than coding
+ The current version calls Arrays.sort(...) and Arrays.parallelSort(...) on an array.
  + Array has roughly 4M array elements.
  + Speedup on the machines I tried was about 2.5 for the parallelSort.
+ In the interest of gathering more data, change the program so that
  it tries various sizes.  For instance:
  #+BEGIN_SRC java
  final int initialSize = 1024;
  final int howMany = 16; // don't go over 20 -- overflow occurs
  int[ ] arraySizes = new int[howMany];
  arraySizes[0] = initialSize;
  for (int i = 1; i < howMany; i++) {
      arraySizes[i] = arraySizes[i - 1] * 2;
  }
  #+END_SRC
  Get the speedup for each of the sizes, and note any trends in speedup.
+ Do the same for numeric types other than int, for instance, long, float, and double.
# * Parallelism And Distributed Computing
# # chapter 12
# + Map/Reduce in Java
# + OpenMPI
 
