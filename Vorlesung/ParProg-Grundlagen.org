#+SETUPFILE: ./theme-bigblow-local.setup
#+TITLE: Parallelprogrammierung -- Grundlagen
#+AUTHOR: Johannes Brauer
#+OPTIONS:   H:4
#+OPTIONS: num:nil d:true
#+OPTIONS: toc:nil
#+OPTIONS: reveal_single_file:nil
#+Language:  de
#+STARTUP: latexpreview
#+STARTUP: inlineimages
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="mycss/mystyle.css" />
# +REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/3.0.0/
#+REVEAL_THEME: simple
#+REVEAL_TRANS: slide
#+REVEAL_HLEVEL: 1
#+REVEAL_INIT_SCRIPT: dependencies: [ { src: 'plugin/menu/menu.js', async: true },
#+REVEAL_INIT_SCRIPT:                 { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true } ]
#+REVEAL_MARGIN: 0.05
#+REVEAL_EXTRA_CSS: ./mycss/myrevealstyle.css
#+OPTIONS: reveal_control:t
# um Folien mit reveal.js erzeugen zu können:ml
# M-x load-library und dann ox-reveal

* Ein wenig Geschichte ...
** ... der Rechensysteme
  (vgl. cite:Bengel2015)
+ 1970 -- Stapelverarbeitung ::
  + Einlesen von Aufträgen (jobs) und Bearbeitung durch die Maschine
  + Job-Scheduler bestimmt die Abarbeitungsreihenfolge
  + /non-blocking IO/ kommt zum Einsatz (Nebenläufigkeit)
+ 1975 -- Timesharing-Systeme ::
  + interaktives Arbeiten mittels Kommandozeilen-Befehlen
  + Mehrbenutzerbetrieb
  + Zuteilung von Zeitscheiben
+ 1980 -- Personal Computer und Workstation ::
  + hohe Rechenleistung am Arbeitsplatz
  + Rückkehr zum Einbenutzerbetrieb
  + Kommandozeile + graphische Benutzungsoberflächen
  + Insellösungen -- kein Zugriff auf gemeinsame Betriebsmittel
  + Notlösung: peer-to-peer-Netze
+ 1985 -- Client-Server-Systeme ::
  + Zentralisierung bestimmter Dienste auf dedizierten Rechnern (server)
  + neue Betriebssystemkonzepte für Netzwerknutzung erforderlich
+ 1990 -- Cluster-Systeme ::
  + auch /Verteilte Systeme/
  + mehrere Rechner bzw. Multiprozessorsysteme teilen sich die Dienstserbringung
  + zur Erhöhung von Leistung und Ausfallsicherheit
  + Load Balancing Cluster / Serverfarmen
  + High Performance Computing (HPC) Cluster
    + Geschwindigkeitssteigerung durch Parallelisierung von Aufgaben
    + bei Ausfall eines Systems übernimmt ein anderes Cluster-Mitglied
      dessen Aufgaben
+ 1995 -- Peer-to-Peer-Systeme ::
  + jede Maschine kann Client und Server sein
  + höhere Ausfallsicherheit
+ 2005 -- Cloud-Systeme oder Cloud-Computing ::
  + Rezentralisierung der Rechenleistung durch Virtualisierung
  + Zugriff auf Ressourcen über Internet oder Intranet
** ... der Technologie
... der Prozessoren und Speicherchips
+ Mooresches Gesetz von 1965: Verdoppelung der Anzahl der Transistoren
  auf einem Chip ca. alle 18 Monate -- gilt noch heute
+ Entwicklung der Mikroprozessoren am Beispiel von Intel
  + 1971: Intel 4004, Transistoren: 2300, Taktrate: 108 kHz
  + 1999: Pentium III, Transistoren: 9.500.000, Taktrate: bis 1 GHz
  + 2000 bis 2008: Pentium 4, Transistoren: 42.000.000, Taktrate: bis
    3,8 GHz
+ Speicherkapazität pro Chip
  + 1970: 1 Kilobit
  + heute: 1 Gigabit
+ Preisverfall bei MIPS und Kosten pro Bit

* Rechnerarchitekturen für parallele und verteilte Systeme
(vgl.  cite:Bengel2015)
** Taxonomie von parallelen Rechnerarchitekturen nach Flynn 
# chapter 10 flynnTax.txt
(vgl. cite:Flynn1972)
+ Anmerkungen:
  + Die Taxonomie sagt nichts darüber aus, ob die Architektur ein
    einzelnes oder ein verteiltes System beschreibt.
  + TDie Taxonomie ist schon recht alt, wird aber als Referenz nach
    wie vor benutzt.
+ Abkürzungen:\\
  D = Daten\\
  I = Befehle (Instruktionen)\\
  P = Prozessor\\
  S steht für /single/, M für /multiple/.
  + Parallelität im Sinne dieser Taxonomie erfordert immer mehrere
    Prozessoren (P).
#+Reveal: split
+ SISD: single instruction, single data ::
  + keine Form von Parallelität
#+BEGIN_EXAMPLE
   +----------+    +---+    +----------+
   | D source |--->| P |<---| I source |                 
   +----------+    +---+    +----------+
#+END_EXAMPLE
+ SIMD: single instruction, multiple data ::
  + Unterstützung in verschiedenen Programmiersprachen
  + heutzutage besonders relevant
#+BEGIN_EXAMPLE
   +----------+    +---+
   | D source |--->| P |<---+
   +----------+    +---+    |
   +----------+    +---+    |  +----------+
   | D source |--->| P |<---+--| I source |               
   +----------+    +---+    |  +----------+
   +----------+    +---+    |
   | D source |--->| P |<---+
   +----------+    +---+
#+END_EXAMPLE   
#+Reveal: split
+ MISD: multiple instruction, single data ::
  + kaum praktische Bedeutung
#+BEGIN_EXAMPLE
                      +---+    +----------+
                 +--->| P |<---| I source |
                 |    +---+    +----------+
   +----------+  |    +---+    +----------+
   | D source |--+--->| P |<---| I source |               
   +----------+  |    +---+    +----------+ 
                 |    +---+    +----------+
                 +--->| P |<---| I source | 
                      +---+    +----------+
#+END_EXAMPLE
+ MIMD: multiple instruction, multiple data ::
  + am ehesten in verteilten Systemen anzutreffen
#+BEGIN_EXAMPLE
   +----------+    +---+    +----------+
   | D source |--->| P |<---| I source |
   +----------+    +---+    +----------+
   +----------+    +---+    +----------+
   | D source |--->| P |<---| I source |                  
   +----------+    +---+    +----------+
   +----------+    +---+    +----------+
   | D source |--->| P |<---| I source |
   +----------+    +---+    +----------+
#+END_EXAMPLE

** Ausprägungen
*** Eng gekoppelte Multiprozessoren und Multicore-Prozessoren
+ mehrere Prozesse können /echt/ parallel ablaufen (auf
  Einprozessorsystemen hingegen nur Quasi-Parallelität)
+ gemeinsamer Hauptspeicher
*** Vektorrechner 
+ Ausführung einer Berechnung gleichzeitig auf vielen Daten
+ Anordnung der Daten als Vektor bzw. Matrix
+ Beispiel für SIMD-Architektur
+ High-Performance-Computing
+ bekannt geworden ursprünglich durch Cray-Supercomputer (seit 1978)
+ genutzt z. B. für Simulationen (Meteorologie)
*** General Purpose Computation on Graphic Processing Unit (GPGPU) 
+ ursprünglich bestehend aus beschränkt programmierbaren
  Spezialprozessoren für Fließkommaoperationen
+ inzwischen frei programmierbare Prozessoren (tausende von Kernen),
  dadurch
+ nutzbar nicht nur für Grafikanwendungen
+ Verwendbarkeit in höheren Programmiersprachen durch spezielle
  Bibliotheken wie z. B. CUDA von Nvidia
+ Beispiel für SIMD-Architektur
*** Lose gekoppelte Multiprozessoren 
+ kein gemeinsamer Speicher
+ Synchronisation und Kommunikation durch Nachrichtenaustausch
+ Ziele: Erhöhung der Leistung, Erhöhung der Verfügbarkeit
+ MIMD möglich


* Parallelität vs. Nebenläufigkeit -- Überblick
# chapter 1 introducton
** Concurrency, parallelism, and non-blocking I/O: an overview

+ For now, concurrency is "handling many tasks during the same time span":
  + Any app with a GUI (e.g., a game) needs to manage the GUI and perform the underlying logic.
  + A typical network app queues up requests, which then need to be handled concurrently.
+ For now, (true) parallelism is "processing many tasks literally at the same time."
  + The tasks in a concurrent app would be farmed out to separate processors for true parallelism.
  + True parallelism requires multiple processors.
    + Multiprocessor machines now are available at consumer prices.
+ Two 'classic' mechanisms for concurrency: multiprocessing and multithreading.
  + Each mechanism supports true parallelism on a multiprocessor machine.
+ Non-blocking I/O enables quick jumps from one I/O task to another, thereby supporting
   concurrency without parallelism.
  + Non-blocking I/O typically is used in conjunction with  a 'classic' approach:
    + The nginx web server: multiprocessing + non-blocking I/O
    + Node.js: non-blocking I/O at the API level, multithreading under the hood.
+ Summary
  + Concurrency is a requirement for many modern apps, and there are various ways to enable it.
  + Parallelism boosts performance by increasing throughput: more work done in a given amount of time.
    + Concurrent tasks can be executed in parallel on a multiprocessor machine.
  + Non-blocking I/O is used mostly in combination with multiprocessing or multithreading
** Concurrency and parallelism: working technical definitions
# chapter 1 concurParall
#+begin_example
      client requests  +------------+   ## Task1: handle request1
    ------------------>| web server |   ## Task2: handle request2
                       +------------+      ...
#+end_example

+ 1st scenario :: a single-processor machine with time-sliced scheduling 
  + Assume each task is scheduled for a fixed-length time slice, and
    then is pre-empted and rescheduled if its processing exceeds that
    time slice.
  #+begin_example 
               Time span (TS) = Start to Finish
Start time                                         Finish time
         |                                         |
         +-----------------------------------------+   ## each star * is one 
          \             /\         /\             /       system-clock tick
           ****Task1****  **Task2**  ****Task1****     ## Task1 (16 ticks) and Task2 
                                                         (4 ticks) share the processor
  #+end_example

  + Task1 and Task2 are processed concurrently during time span TS: their processing overlaps during TS.
    + Task1 and Task2 are not processed in parallel, as there's but a single processor that must be shared.
  + 1st scenario exhibits concurrency, but not true parallelism.

+ 2nd scenario :: a multi-processor machine (assume two for simplicity)
  #+begin_example
                 Time span 
Start time                       Finish time
         |                       |
         +-----------------------+
          \                     /
           ********Task1********      ### Executing on processor1
		   \         /
		    **Task2**         ### Executing on processor2
  #+end_example

  + Task1 and Task2 again are processed concurrently (during the same time span TS).
  + Task1 and Task2 also are processed in parallel for a while--for 4
    ticks (Task2's processing time).
  + The machine's throughput is improved: Task1 and Task2 both
    complete within 16 time units, the time taken to process Task1,
    the longer of the two.
    + 1st scenario requires 20 clock ticks (each 1 time unit) to complete the two tasks.
    + 2nd scenario requires 16 clock ticks to complete the two tasks, a 20% improvement.
** System call overview for concurrency, parallelism, and non-blocking I/O
# chapter 1 syscall.txt
+ The systems context: user-space and kernel-space code
  + User-space code does not control shared physical resources (processors, memory, I/O devices).
    + Ordinary applications execute in user space.
  + Kernel-space code comprises the core OS routines that control shared physical resources.
  + A "system call" originates in user-space, but results in the execution of a kernel-space routine.
    + Standard library functions (user-space) mediate between ordinary
      application code and the core OS routines (kernel-space) that
      some library functions call.
    + The standard library goes by various names: on Unix-type
      systems, libc or variants thereof (e.g., glibc on Ubuntu); on
      Windows, the Windows API or variants thereof (e.g., Win32 API)
  + Depiction: Node.js app calls the cluster.fork() function
  #+BEGIN_EXAMPLE
                                                            system call
                                                 user space<----+---->kernel space
                                                                |
            calls                 calls                         | calls
Node.js app------->cluster.fork()------->library function fork()|------->kernel routine 
                                                                |     ## in Unix: fork
                                                                |     ## in Linux: clone
  #+END_EXAMPLE
+ Importance of C: The standard libraries and kernel routines are
     written mostly in C, with some assembly language or non-standard
     C extensions covering the rest.
** Wrapup of the overview
# chapter 1 wrapup1.txt
+ Concurrency and parallelism are different.
  + Concurrency involves handling multiple tasks in the same time span.
  + Parallelism involves processing multiple tasks literally at the same time.
    + True parallelism requires multiple processors.
  + Parallelism is a performance-boosting way to manage concurrency.
+ The classic approaches to concurrency are multiprocessing and multithreading.
  + Non-blocking I/O has emerged as an option as well, but one that's
    typically used in conjunction with multiprocessing or
    multithreading.
  + Non-blocking I/O sometimes is described as 'concurrency without parallelism'.
+ Different languages have difference advantages when it comes to
  studying concurrency, parallelism, and non-blocking I/O.
  + C is close to the metal: the system libraries and core OS routines at work under the hood.
  + Whatever the application language, multiprocessing,
    multithreading, and non-blocking I/O calls in the language hit the
    C libraries and, from there, kernel-space routines.
  + Java (C#), Go, Node.js and other languages have particular styles
    and strengths with respect to concurrency in particular.
* Parallelität vs. Nebenläufigkeit im Detail
** Processes and threads 
# chapter 2: processThread.txt
+ A process is a 'program in execution', each with its own address space.
  + Executing a program from the command-line is an example:
      =% hi.exe=  ## =%= is the command-line prompt, =hi.exe= is an executable program.
  + The address space comprises the memory locations accessible to the process.
  + Separate address spaces effectively partition memory among the processes.
+ A thread is a sequence of executable instructions within a process.
  + A 1-instruction thread is trivial, and threads made up of tens of
     instructions are more typical.
  + A single-threaded process has one thread, whereas a multithreaded process has multiple ones.
  + Although there are different ways to implement threads (more on this later), the
     critical point is this:
    + Threads within a process share the same address space--they have
      access to exactly the same memory locations. This is the root
      cause of race conditions.
    + A 'race condition' arises when two or more threads concurrently
      access the same memory location and at least one of the threads
      tries to update the location. For instance, each thread might be
      executing the instruction:\\
         =n = random_num()=  ## =n= is a shared memory location among the threads, random_num() returns a random number\\
         The result is unpredictable, except that the last thread to execute this 'wins'.
+ Scheduling on modern systems: to schedule a process on a processor is to schedule one of its threads.
#+begin_example
  Process1    scheduled   +----------+
    Thread11------------->|processor3|
    Thread12              +----------+

  Process2
    Thread21
    Thread22  scheduled   +----------+
    Thread23------------->|processor7|
                          +----------+
#+end_example
** Multiprocessing
# chapter 2 multiprocessing.txt
A first look at multiprocessing: the command-line pipe operator =|=
+ At the command line:
  : cat file.txt | sort
  + The 'cat' (short for 'concatenate') on the left is one executing
    process, and the 'sort' on the right is another executing process.
  + The 'sort' performs blocking I/O: it waits for all of the bytes
    from 'cat' before doing the sorting.
  + If the machine has two processorss, 'cat' could execute on one and 'sort' on the other.
  + The unnamed pipe operator | thus performs automatic multiprocessing.
    + A pipe is thus a mechanism for inter-process communication (IPC).
+ Conceptual view of the pipe:
#+begin_example
                       pipe
               +--------|-------+
    writer---->|WriteEnd|ReadEnd|<----reader
               +--------+-------+
#+end_example
+ The tasks at hand (producing the bytes in the file and then sorting
  the lines) are divided between two proceses, 'cat' and 'sort'.
+ Two general approaches in code:
  + A parent process 'forks' (clones, spawns) a child process, and both
      execute code from the same program.	  
      * An if-else or the equivalent typically separates the parent code from the child code.
  + A parent process forks a child, which then executes a separate program.
+ A final example to underscore the point:
  : sleep 5 | echo 'Hello, world!'  ## sleep sends no bytes to the pipe, and echo expects none
  + Hello, world! appears on the screen, and roughly 5 seconds later the prompt returns from the sleep command.
** Multithreading
# chapter 2: multithreading.txt
A first look at multithreading: the Tomcat web server, written in Java

+ Tomcat implements the 'one-thread-per-request' model for handling client requests.
+ Tomcat runs as a single process, which is multithreaded.
+ Web sites and services are deployed as JAR files (with a .war
  extension) dropped into a particular directory (TOMCAT_HOME/webapps)
  or subdirectory thereof.
  + The WAR file's name (e.g., 'site1' in 'site1.war') becomes the first part of 'resource path'
      in a request URL:\\

      http://somemachine.org:8080/site1/...    ## site1 for the site1.war file\\
      http://somemachine.org:8080/site1/hi.jsp ## a completed URL, with hi.jsp as the requested resourc
  + Tomcat delegates each request for any WAR file to a thread, the 'one-thread-per-request' model
    + Code within the WAR file is thus susceptible to race conditions
      because multiple request-handling threads have access to the
      very same memory locations: the burden is on the programmer to
      ensure proper thread coordination.
+ For efficiency, Tomcat creates a thread pool at start-up: indeed, two thread pools--one for
   requests over HTTP, another for requests over HTTPS if HTTPS is enabled.
+ Other Java-centric web servers, such as Jetty, implement the one-thread-per-request model as well.
** Non-blocking I/O
A first look at non-blocking I/O non-blocking I/O: two code examples

+ A blocking 'getItem' (read) example
#+BEGIN_SRC html
    <!doctype html>
    <html>
      <head>
        <title>Blocking I/O</title>
        <script>
          localStorage.name = 'Flintstone';             <!-- line 1 -->
          alert('Before blocking call');                <!-- line 2 -->
          var value = localStorage.getItem('name');     <!-- line 3 -->
          alert('After blocking call: ' + value);       <!-- line 4 -->
        </script>
      </head>
      <body/>
    </html>
#+END_SRC 
  * The second alert occurs only after the blocking call to 'getItem' (line 3) returns.

+ A non-blocking 'fetch' example in pseudo-code
#+BEGIN_SRC java
    define callbackFunction(Http-response) {
       productList = extractProductListFromResponse(Http-response)
       print(productList)                            ## print is blocking
    }

    // Main code 
    url = 'http://some-machine.org/some-script-to-get-data'
    connection = getConnectionToRemoteMachine(url)
    connection.fetch(callbackFunction)                   ## fetch(...) is non-blocking
    print('Hello, world!')                               ## print is blocking
    print('Goodbye, cruel world!')                       ## print is blocking
#+END_SRC
** Zusammenfassung 
 Three approaches to concurrency: multiprocessing, multithreading, non-blocking I/O

#+begin_example 
     client requests  +------------+   ## Task1: handle request1
   ------------------>| web server |   ## Task2: handle request2
                      +------------+      ...
#+end_example

+ Multiprocessing: dispatch each task to a separate process ('program in execution')
  + For efficiency, "prefork" the processes by building a pool of these
   at start-up.  Then grab a process from the pool to act as the 'task
   handler'.
  + When the task-handling process finishes its work, put it to sleep.
  + Apache2, nginx, and IIS are production-grade examples.

+ Multithreading: dispatch each task to a separate thread within a process.
  + Again for efficiency, build a thread-pool at start-up, and grab a
    thread from the pool to act as the 'task handler'.
   + When the task-handling thread finishes its work, put it to sleep.
   + Tomcat and Jetty are production-grade examples.
+ Non-blocking I/O: in its pure form, a single-threaded process that
  jumps quickly from one task to another, perhaps doing only partial
  processing of each task.
  + For example, the non-blocking web server might read (and cache)
    some bytes as part of Task1, then do the same for Task2, and so on
    until all of the request bytes for a given task have been read.
   + Node.js is the obvious 'pure' example (but even Node.js takes a hybrid approach).
** Hybrid approaches
# chapter 2: hybrid.txt
+ There's no hard-and-fast rule about how best to support concurrency.
  + Multiprocessing, multithreading, and non-blocking I/O can be used
    in various combinations.
+ Sample hybrids
  + Multiprocessing + multithreading: IIS (Windows web server) and
    AspNet runtime
  #+begin_example
  client side                         server side
+-------------+  HTTP requests    +-----------------+
| web browser |------------------>| Web server (IIS)|
|             |<------------------|                 |
+-------------+  HTTP responses   +-------+---------+
                                          |
                                    aspnet_isapi.dll    ## Links IIS process with workers
                                          |
                                          +--- aspnet_wp.exe    ## worker process 1
                                          |
                                          +--- aspnet_wp.exe    ## worker process 2
                                          ...                   ...
  #+end_example
    + Each worker process is multithreaded (by default, 10 threads per
      worker process), and each thread handles a single request (the
      'one-thread-per-request' model)
  + Multiprocessing + non-blocking I/O: nginx
    + Multiprocessing:
      + nginx has a 'master process' to read configuration files and
        to watch over worker processes (e.g., start a new worker if
        one happens to die unexpectedly).
      + A 'worker process' handles a client request: a given worker
        may handle even thousands of requests concurrently--given the
        efficiency of non-blocking I/O.
      + Other (optional) processes (e.g., a cache loader and a cache
        manager) also are in play.
      + Non-blocking I/O:
        + The workers employ non-blocking I/O in handling requests,
          each of which is partitioned into a collection of
          sub-requests. A completed sub-request generates an event,
          which is handled in the course of assembling a full response
          out of pieces.
  + Non-blocking I/O + multithreading (under the hood): Node.js
  #+begin_example
               Node.js server  ## a single-threaded process managing an event loop
client request  +--------+
--------------->|        |     ## 'workers' are JavaScript functions that read and
     ...        | event           otherwise handle a client's request using non-blocking 
client request  | loop   |        I/O and callbacks to signal task completion
--------------->|        |
                +--------+     ## Long-running tasks (e.g., accessing a DB) are delegated 
			          to under-the-hood threads, with callbacks to signal
                                  task completion
  #+end_example

* Multiprocessing -- Grundlagen
** A first multiprocessing example in C
*** Overview
# chapter 3 firstExample.txt
... key points of the 'unnamed pipe example'

+ When the program pipeExample.c is compiled and then executed from
  the command-line, it executes as a process. Here's the command to
  execute:
  : % ./pipeExample   ## On Windows: % pipeExample
+ The executing pipeExample is the 'parent' process.
+ The parent process calls the library function fork() to spawn a
  'child' process, a clone of the parent.
  + The library function fork() makes a system call to the OS routine that spawns the child.
  + The library function fork() returns one of three values:
    + -1 :: to signal failure (e.g., the system already has too
            many processes)
    + 0 ::  to the child process
    + > 0 :: to the parent process; in particular, the 'process id'
             (pid) of the child is returned to the parent, and a pid
             is always a positive integer
  + The parent and child process both execute the code that follows the call to fork().
    + fork() effectively 'clones' one process (child) from another (parent).
    + An if-else or equivalent construct is used to ensure that the
      parent does one thing and the child does something else.
      #+BEGIN_SRC C
pid_t pid = fork();   /** pid_t is a signed integer data type **/
if (-1 == pid) {      /** error: couldn't clone a new process **/
    ...                
}   
else if (0 == pid) {  /** child code follows **/
    ...
}
else {                /** parent code follows **/
    ...
}
      #+END_SRC
  + The parent waits for the child to exit so that the child does not
    become a 'zombie'.
    + When a child terminates, the OS notifies the parent. If the
      parent is already terminated, it cannot be notified--and the
      child becomes an 'zombie', dead but still around.
*** using the shell...
#+BEGIN_SRC sh
bash-3.2$ ls pipeExample.c 
pipeExample.c
bash-3.2$ gcc -o pipeExample pipeExample.c 
bash-3.2$ ./pipeExample
This is the winter of our discontent
bash-3.2$ 
#+END_SRC
*** Source code
#+BEGIN_SRC C
/** A first multiprocessing example: pipes in C 
    To compile (Unix-like system): gcc -o pipeExample pipeExample.c
    For Windows, the GNU compiler: https://gcc.gnu.org/
**/
#include <sys/wait.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>

#define ReadEnd  0                                                  /** line 1 **/            
#define WriteEnd 1                                                

void report_and_die(const char* msg) {                              /** line 2 **/
  perror(msg);
  exit(-1);    /** failure **/
}

int main() {
  int pipeFDs[2];                                                   /** line 3 **/
  char buf;         /* 1-byte buffer */
  const char* msg = "This is the winter of our discontent\n";

  if (pipe(pipeFDs) < 0) report_and_die("pipe(...)");               /** line 4 **/

  pid_t cpid = fork();                                              /** line 5 **/
  if (cpid < 0) report_and_die("fork()");                           /** line 6 **/

  /*** child ***/
  if (0 == cpid) {                                                  /** line 7 **/
    close(pipeFDs[WriteEnd]);                                       /** line 8 **/
    while (read(pipeFDs[ReadEnd], &buf, 1) > 0)                     /** line 9 **/
      write(STDOUT_FILENO, &buf, sizeof(buf));                      /** 1ine 10 **/
    close(pipeFDs[ReadEnd]);                                        /** line 11 **/
    _exit(0); /* signal parent of immediate exit **/                /** line 12 **/
  } 
  /*** parent ***/
  else {                                                            /** line 13 **/ 
    close(pipeFDs[ReadEnd]);                                   
    write(pipeFDs[WriteEnd], msg, strlen(msg));                     /** line 14 **/
    close(pipeFDs[WriteEnd]); /* generates an EOF */                /** line 15 **/
    wait(0); /*** wait for child to exit ***/                       /** line 16 **/  
    exit(0); /* exit normally */                                    /** line 17 **/
  }
  return 0;
}
#+END_SRC
** Second Example -- Named Pipes
*** Overview
# chapter 3 secondExample.txt
 A second multiprocessing example in C: key points of the 'named pipe' example

+ Two separate processes, but no parent/child relationship between them.
  + The processes communicate through a 'named pipe' (aka 'FIFO').
    + A named pipe is a mechanism designed to support IPC among processes on the same host.
      #+begin_example
           +------------+
writer---->| named pipe |<----reader
           +------------+
      #+end_example
+ The system implements an FIFO as a temporary file.
+ Named pipes are quite similar in functionality to Unix Domain
  Sockets, also called 'local sockets'.
  + Network sockets support communcation between processes running on
    different hosts, whereas named pipes and local sockets do the same
    for processes running on the same host.
+ The example consists of two executable programs, fifoWriter and
  fifoReader, which can be executed at the command-line as follows:
  : % ./fifoWriter &    ## for Windows: fifoWriter & (& puts the process in the background) 
  : % ./fifoReader      ## for Windows: fifoReader
  + There are now two processes executing: fifoWriter and fifoReader
  + The fifoWriter writes randomly generated integers to the FIFO.
  + The fifoReader reads these integers from the FIFO until end-of-file.
  + In the C code, the FIFO is 'written' and 'read' as if it were a
    regular disk file: the I/O API is the same.
*** using the shell
#+BEGIN_SRC sh
bash-3.2$ ./fifoWriter &
[1] 7316
bash-3.2$ ./fifoReader
60749779
966803328
1202260494
...
[1]+  Done                    ./fifoWriter
bash-3.2$ 
#+END_SRC
*** Source Code =fifoWriter.c=
#+BEGIN_SRC C
#include <fcntl.h> 
#include <stdio.h>
#include <time.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/stat.h>
#include <sys/types.h>

#define MaxWrites 256  

int main() {
  srand(time(0)); /* seed the random number generator */

  const char* fifoName = "./myPipe1";          
  mkfifo(fifoName, 0666); /* read/write for user/group/others */             /** line 1 **/
  int fd = open(fifoName, O_CREAT | O_WRONLY); /* open blocks on a FIFO */   /** line 2 **/

  int i;
  for (i = 0; i < MaxWrites; i++) { 
    int n = rand();             
    write(fd, &n, sizeof(int));  /* 4 bytes per int */                       /** line 3 **/
  }
  close(fd);         /* close the fifo */                                    /** line 4 **/
  unlink(fifoName);  /* dispose of underlying file */                        /** line 5 **/
  return 0;
}
#+END_SRC
*** Source Code =fifoReader.c=
#+BEGIN_SRC C
#include <fcntl.h> 
#include <stdio.h>
#include <time.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/stat.h>
#include <sys/types.h>

int main() {
    const char* fifoName = "./myPipe1";
    int num;
    int fd = open(fifoName, O_RDONLY);              /** line 1 **/

    while (read(fd, &num, sizeof(num)) > 0)         /** line 2 **/ 
      printf("%i\n", num);
    close(fd);                                      /** line 3 **/
    unlink(fifoName);                               /** line 4 **/
    return 0;
}
#+END_SRC
** The Nginx Web Server
 Multiprocessing in production-grade software: the nginx web server

+ www.nginx.com
  + The web server is written in C.
  + nginx combines multiprocessing and non-blocking I/O as concurrency mechanisms.

+ nginx, like any modern web server that relies on multiprocessing for concurrency,
   is a "pre-forking" server.
  + nginx can be started as a 'system service' (at boot time), and is platform-neutral.

+ =nginx.conf=  ()configuration entries are key/value pairs: no XML!)
  #+begin_example 
user www-data;           ## www-data (name is arbitrary) is owner of workers
worker_processes 4;      ## default number of workers, might be set to processor count
pid /var/run/nginx.pid;  ## file /var/run/nginx.pid stores pid of master process

events {
   worker_connections 1024;  ## Max of 1024 connections per worker 
   ...                       ## (typical browsers open at least two connections to a 
                                site per client session)
    }   
  #+end_example

+ =ps -ef | grep nginx=
  #+begin_example 
Owner     pid   ppid   nginx process type

root      1803     1   master process /usr/sbin/nginx  ## command that started nginx
www-data  1804  1803   worker process
www-data  1805  1803   worker process
www-data  1807  1803   worker process
www-data  1808  1803   worker process
  #+end_example
** Execing and Forking Options in Multiprocessing
# chapter 3 execOverview.txt
 A second way for one executing program to start another

+ Review of the previously way, introduction of the new way:
  #+begin_example
        fork()
parent---------->child        ## Both parent and child continue to execute...

         exec...()
process1----------->process2  ## process2 replaces process1, which terminates
  #+end_example
+ In systems speak, an 'image' is an executable program. For instance,
  copying the system from one machine to another is called
  're-imaging'.
  + In the multiprocessing 'pipeExample' code, the image is the
    compiled, executable program named 'pipeExample'
+ A new process can be 'spawned' (created) under one of two distinct
  scenarios, which the difference between the fork() function and the
  exec-family of functions illustrates.
  + The exec-family consists of library functions that, under the
    hood, make the same system call.
+ In a successful call to fork(), the new child process executes the
  'same' image as the parent that spawned it: the forked process is a
  clone. So the standard code idiom is this:
  #+BEGIN_SRC C
pid_t pid = fork();  /* parent executes this */
...                  /* check for success */
if (0 == pid) {      /* 0 returned to the child */
   ...               /* child code */
}
else {               /* parent code: fork returns child's pid to parent */
   ...               /* alternative parent test: if (pid > 0)
}
  #+END_SRC
+ In a successful call to an exec-function, replaces the current process image with
   a new image that's loaded into memory.
  #+BEGIN_SRC c
int flag = execl("/usr/bin/myGame", "myGame", 0);
if (-1 == flag) perror("Couldn't exec...");
/* on success, the 'myGame' program now executes */
  #+END_SRC
  + The new image has its own address space and other features. In
    summary, the old process stops execution and a new one begins
    execution.
  + The new process retains the pid (and ppid) of the one it replaces
    + The user and group properties are likewise unchanged.
*** Using the Shell
#+BEGIN_SRC sh
bash-3.2$ ./fileStatus tmp
File name: tmp

Information about file tmp:

Owner ID:    503
Group ID:    20
Byte size:   68
Last access: Fri Oct  6 11:10:23 2017
File type:   directory
Owner read:  Owner readable
Owner write: Owner writable

bash-3.2$ ./fileStatus fileStatus
File name: fileStatus

Information about file fileStatus:

Owner ID:    503
Group ID:    20
Byte size:   8808
Last access: Fri Oct  6 11:12:05 2017
File type:   regular
Owner read:  Owner readable
Owner write: Owner writable

bash-3.2$ ./execing
File name: tmp

Information about file tmp:

Owner ID:    503
Group ID:    20
Byte size:   68
Last access: Fri Oct  6 11:10:23 2017
File type:   directory
Owner read:  Owner readable
Owner write: Owner writable
#+END_SRC
*** Source Code
#+BEGIN_SRC C
/* To run: 

   -- compile this program (gcc -o execing execing.c) and fileStatus.c 
   --                      (gcc -o fileStatus fileStatus.c)
   -- create a subdirectory named tmp: mkdir tmp
   -- from the command line: ./execing

   This program 'execs' the fileStatus program, which outputs metadata about the tmp dir..
   Here's a depiction:

                   execv
   execing program------->fileStatus program
*/

#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>

int main() {
  char* const args[] = {"./fileStatus", "tmp", 0};  /* fileStatus program, tmp directory */
  int ret = execv("./fileStatus", args);            /* "v" for "vector" */
  if (-1 == ret) {                                  /* check for failure */
    perror("execv(...) error");
    exit(-1);
  }
  else 
    printf("I'm here\n");                           /* never executes */
  return 0;
}

#+END_SRC
** Process Tracking and Management
# chapter 3 processTracking.txt
 Process tracking and context switching

+ Each process has a unique ID (pid), a non-negative integer.
  + A process's pid is recorded in the system's process table.
  + Library functions getpid() and getppid() get the pid and the
    parent's pid, respectively.
  + The pid of 0 is typically reserved for the 'idle process', the one
    that 'runs' when there's nothing else to run.
  + The 'init process' has a pid of 1: the first user-space process
    that the OS kernel spawns in the boot-up.
    + On shutdown, the init process is the last process to terminate:
      it waits for all of its children.
    + The init process starts other processes: system services such as
      the login service, etc.
+ Key data structures in a process 'context':
  + process table: tracks information per process, in particular
    information that allows a pre-empted process to restart later.
  + page table(s): translate virtual into physical addresses. Each
    process has its own, which effectively partitions virtual address
    spaces among processes.
  + file table: tracks, per process, files that a process has opened.
+ Every process has a user and group 'owner', which determines the
  process's access rights to resources.
  + Every process likewise belongs to a 'process group', which
    facilitates sending signals to every process in a group, to
    getting information about related processes, and so on.
  + In the default case, a child process belongs to the same as its
    parent.
+ Most modern systems use pre-emptive scheduling, with either
  fixed-length or variable-length time-slices: a process that exceeds
  the time-slice on a given run is pre-empted and sent back to a
  scheduling queue.
  + A process and a thread 'context' consists of key information about
    it.
    + For a process, key data structures such as the process table,
      the file table, and its page tables define the 'context'.
+ Context switches, which occur during pre-emption, come in two
  flavors:
  + From one thread to another within the same process: 'intraprocess
    switching'
  + From one process to another: 'interprocess switching'
    + The big cost is the swapping out of one (virtual) address space
      for another: page tables
  + A process-level context switch is signficantly more expensive than
    a thread-level context switch.
    + The cost of thread-level switching is near zero on modern systems.
    + Depiction:
      #+begin_example
Process1      Process2        processor7
  Thread11      Thread21
  Thread12

1st scenario: Thread12 replaces Thread11 on processor7  + virtually no overhead

2nd scenario: Thread21 replaces Thread11 on processor7  + high overhead--
                                                          process context switch
      #+end_example

* weitere verwendete Literatur
+ cite:CACM2017
+ cite:CACM2016
+ cite:Subramanian2017
* bibliography:referenzen.bib
 
