#+SETUPFILE: ./theme-bigblow-local.setup
#+TITLE: Parallelprogrammierung -- Grundlagen
#+AUTHOR: Johannes Brauer
#+OPTIONS:   H:4
#+OPTIONS: num:nil d:true
#+OPTIONS: toc:nil
#+OPTIONS: reveal_single_file:nil
#+Language:  de
#+STARTUP: latexpreview
#+STARTUP: inlineimages
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="mycss/mystyle.css" />
# +REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/3.0.0/
#+REVEAL_THEME: simple
#+REVEAL_TRANS: slide
#+REVEAL_HLEVEL: 1
#+REVEAL_INIT_SCRIPT: dependencies: [ { src: 'plugin/menu/menu.js', async: true },
#+REVEAL_INIT_SCRIPT:                 { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true } ]
#+REVEAL_MARGIN: 0.05
#+REVEAL_EXTRA_CSS: ./mycss/myrevealstyle.css
#+OPTIONS: reveal_control:t
# um Folien mit reveal.js erzeugen zu können:ml
# M-x load-library und dann ox-reveal

* Ein wenig Geschichte ...
** ... der Rechensysteme
  (vgl. cite:Bengel2015)
+ 1970 -- Stapelverarbeitung ::
  + Einlesen von Aufträgen (jobs) und Bearbeitung durch die Maschine
  + Job-Scheduler bestimmt die Abarbeitungsreihenfolge
  + /non-blocking IO/ kommt zum Einsatz (Nebenläufigkeit)
+ 1975 -- Timesharing-Systeme ::
  + interaktives Arbeiten mittels Kommandozeilen-Befehlen
  + Mehrbenutzerbetrieb
  + Zuteilung von Zeitscheiben
+ 1980 -- Personal Computer und Workstation ::
  + hohe Rechenleistung am Arbeitsplatz
  + Rückkehr zum Einbenutzerbetrieb
  + Kommandozeile + graphische Benutzungsoberflächen
  + Insellösungen -- kein Zugriff auf gemeinsame Betriebsmittel
  + Notlösung: peer-to-peer-Netze
+ 1985 -- Client-Server-Systeme ::
  + Zentralisierung bestimmter Dienste auf dedizierten Rechnern (server)
  + neue Betriebssystemkonzepte für Netzwerknutzung erforderlich
+ 1990 -- Cluster-Systeme ::
  + auch /Verteilte Systeme/
  + mehrere Rechner bzw. Multiprozessorsysteme teilen sich die Dienstserbringung
  + zur Erhöhung von Leistung und Ausfallsicherheit
  + Load Balancing Cluster / Serverfarmen
  + High Performance Computing (HPC) Cluster
    + Geschwindigkeitssteigerung durch Parallelisierung von Aufgaben
    + bei Ausfall eines Systems übernimmt ein anderes Cluster-Mitglied
      dessen Aufgaben
+ 1995 -- Peer-to-Peer-Systeme ::
  + jede Maschine kann Client und Server sein
  + höhere Ausfallsicherheit
+ 2005 -- Cloud-Systeme oder Cloud-Computing ::
  + Rezentralisierung der Rechenleistung durch Virtualisierung
  + Zugriff auf Ressourcen über Internet oder Intranet
** ... der Technologie
... der Prozessoren und Speicherchips
+ Mooresches Gesetz von 1965: Verdoppelung der Anzahl der Transistoren
  auf einem Chip ca. alle 18 Monate -- gilt noch heute
+ Entwicklung der Mikroprozessoren am Beispiel von Intel
  + 1971: Intel 4004, Transistoren: 2300, Taktrate: 108 kHz
  + 1999: Pentium III, Transistoren: 9.500.000, Taktrate: bis 1 GHz
  + 2000 bis 2008: Pentium 4, Transistoren: 42.000.000, Taktrate: bis
    3,8 GHz
+ Speicherkapazität pro Chip
  + 1970: 1 Kilobit
  + heute: 1 Gigabit
+ Preisverfall bei MIPS und Kosten pro Bit

* Rechnerarchitekturen für parallele und verteilte Systeme
(vgl.  cite:Bengel2015, cite:Kalin2015)
** Taxonomie von parallelen Rechnerarchitekturen nach Flynn 
# chapter 10 flynnTax.txt
(vgl. cite:Flynn1972)
+ Anmerkungen:
  + Die Taxonomie sagt nichts darüber aus, ob die Architektur ein
    einzelnes oder ein verteiltes System beschreibt.
  + TDie Taxonomie ist schon recht alt, wird aber als Referenz nach
    wie vor benutzt.
+ Abkürzungen:\\
  D = Daten\\
  I = Befehle (Instruktionen)\\
  P = Prozessor\\
  S steht für /single/, M für /multiple/.
  + Parallelität im Sinne dieser Taxonomie erfordert immer mehrere
    Prozessoren (P).
#+Reveal: split
+ SISD: single instruction, single data ::
  + keine Form von Parallelität
#+BEGIN_EXAMPLE
   +----------+    +---+    +----------+
   | D source |--->| P |<---| I source |                 
   +----------+    +---+    +----------+
#+END_EXAMPLE
+ SIMD: single instruction, multiple data ::
  + Unterstützung in verschiedenen Programmiersprachen
  + heutzutage besonders relevant
#+BEGIN_EXAMPLE
   +----------+    +---+
   | D source |--->| P |<---+
   +----------+    +---+    |
   +----------+    +---+    |  +----------+
   | D source |--->| P |<---+--| I source |               
   +----------+    +---+    |  +----------+
   +----------+    +---+    |
   | D source |--->| P |<---+
   +----------+    +---+
#+END_EXAMPLE   
#+Reveal: split
+ MISD: multiple instruction, single data ::
  + kaum praktische Bedeutung
#+BEGIN_EXAMPLE
                      +---+    +----------+
                 +--->| P |<---| I source |
                 |    +---+    +----------+
   +----------+  |    +---+    +----------+
   | D source |--+--->| P |<---| I source |               
   +----------+  |    +---+    +----------+ 
                 |    +---+    +----------+
                 +--->| P |<---| I source | 
                      +---+    +----------+
#+END_EXAMPLE
+ MIMD: multiple instruction, multiple data ::
  + am ehesten in verteilten Systemen anzutreffen
#+BEGIN_EXAMPLE
   +----------+    +---+    +----------+
   | D source |--->| P |<---| I source |
   +----------+    +---+    +----------+
   +----------+    +---+    +----------+
   | D source |--->| P |<---| I source |                  
   +----------+    +---+    +----------+
   +----------+    +---+    +----------+
   | D source |--->| P |<---| I source |
   +----------+    +---+    +----------+
#+END_EXAMPLE

** Ausprägungen
*** Eng gekoppelte Multiprozessoren und Multicore-Prozessoren
+ mehrere Prozesse können /echt/ parallel ablaufen (auf
  Einprozessorsystemen hingegen nur Quasi-Parallelität)
+ gemeinsamer Hauptspeicher
*** Vektorrechner 
+ Ausführung einer Berechnung gleichzeitig auf vielen Daten
+ Anordnung der Daten als Vektor bzw. Matrix
+ Beispiel für SIMD-Architektur
+ High-Performance-Computing
+ bekannt geworden ursprünglich durch Cray-Supercomputer (seit 1978)
+ genutzt z. B. für Simulationen (Meteorologie)
*** General Purpose Computation on Graphic Processing Unit (GPGPU) 
+ ursprünglich bestehend aus beschränkt programmierbaren
  Spezialprozessoren für Fließkommaoperationen
+ inzwischen frei programmierbare Prozessoren (tausende von Kernen),
  dadurch
+ nutzbar nicht nur für Grafikanwendungen
+ Verwendbarkeit in höheren Programmiersprachen durch spezielle
  Bibliotheken wie z. B. CUDA von Nvidia
+ Beispiel für SIMD-Architektur
*** Lose gekoppelte Multiprozessoren 
+ kein gemeinsamer Speicher
+ Synchronisation und Kommunikation durch Nachrichtenaustausch
+ Ziele: Erhöhung der Leistung, Erhöhung der Verfügbarkeit
+ MIMD möglich
* Parallelität vs. Nebenläufigkeit -- Überblick
# chapter 1 introducton
** Concurrency, parallelism, and non-blocking I/O: an overview
+ For now, concurrency is "handling many tasks during the same time span":
  + Any app with a GUI (e.g., a game) needs to manage the GUI and perform the underlying logic.
  + A typical network app queues up requests, which then need to be handled concurrently.
+ For now, (true) parallelism is "processing many tasks literally at the same time."
  + The tasks in a concurrent app would be farmed out to separate processors for true parallelism.
  + True parallelism requires multiple processors.
    + Multiprocessor machines now are available at consumer prices.
+ Two /classic/ mechanisms for concurrency: multiprocessing and multithreading.
  + Each mechanism supports true parallelism on a multiprocessor machine.
+ Non-blocking I/O enables quick jumps from one I/O task to another, thereby supporting
   concurrency without parallelism.
  + Non-blocking I/O typically is used in conjunction with  a /classic/ approach:
    + The nginx web server: multiprocessing + non-blocking I/O
    + Node.js: non-blocking I/O at the API level, multithreading under the hood.
+ Summary
  + Concurrency is a requirement for many modern apps, and there are various ways to enable it.
  + Parallelism boosts performance by increasing throughput: more work done in a given amount of time.
    + Concurrent tasks can be executed in parallel on a multiprocessor machine.
  + Non-blocking I/O is used mostly in combination with multiprocessing or multithreading
** Concurrency and parallelism: working technical definitions
# chapter 1 concurParall
#+begin_example
      client requests  +------------+   ## Task1: handle request1
    ------------------>| web server |   ## Task2: handle request2
                       +------------+      ...
#+end_example

+ 1st scenario :: a single-processor machine with time-sliced scheduling 
  + Assume each task is scheduled for a fixed-length time slice, and
    then is pre-empted and rescheduled if its processing exceeds that
    time slice.
  #+begin_example 
               Time span (TS) = Start to Finish
Start time                                         Finish time
         |                                         |
         +-----------------------------------------+   ## each star * is one 
          \             /\         /\             /       system-clock tick
           ****Task1****  **Task2**  ****Task1****     ## Task1 (16 ticks) and Task2 
                                                         (4 ticks) share the processor
  #+end_example

  + Task1 and Task2 are processed concurrently during time span TS: their processing overlaps during TS.
    + Task1 and Task2 are not processed in parallel, as there's but a single processor that must be shared.
  + 1st scenario exhibits concurrency, but not true parallelism.

+ 2nd scenario :: a multi-processor machine (assume two for simplicity)
  #+begin_example
                 Time span 
Start time                       Finish time
         |                       |
         +-----------------------+
          \                     /
           ********Task1********      ### Executing on processor1
		   \         /
		    **Task2**         ### Executing on processor2
  #+end_example

  + Task1 and Task2 again are processed concurrently (during the same time span TS).
  + Task1 and Task2 also are processed in parallel for a while--for 4
    ticks (Task2's processing time).
  + The machine's throughput is improved: Task1 and Task2 both
    complete within 16 time units, the time taken to process Task1,
    the longer of the two.
    + 1st scenario requires 20 clock ticks (each 1 time unit) to complete the two tasks.
    + 2nd scenario requires 16 clock ticks to complete the two tasks, a 20% improvement.
** System call overview for concurrency, parallelism, and non-blocking I/O
# chapter 1 syscall.txt
+ The systems context: user-space and kernel-space code
  + User-space code does not control shared physical resources (processors, memory, I/O devices).
    + Ordinary applications execute in user space.
  + Kernel-space code comprises the core OS routines that control shared physical resources.
  + A "system call" originates in user-space, but results in the execution of a kernel-space routine.
    + Standard library functions (user-space) mediate between ordinary
      application code and the core OS routines (kernel-space) that
      some library functions call.
    + The standard library goes by various names: on Unix-type
      systems, libc or variants thereof (e.g., glibc on Ubuntu); on
      Windows, the Windows API or variants thereof (e.g., Win32 API)
  + Depiction: Node.js app calls the cluster.fork() function
  #+BEGIN_EXAMPLE
                                                            system call
                                                 user space<----+---->kernel space
                                                                |
            calls                 calls                         | calls
Node.js app------->cluster.fork()------->library function fork()|------->kernel routine 
                                                                |     ## in Unix: fork
                                                                |     ## in Linux: clone
  #+END_EXAMPLE
+ Importance of C: The standard libraries and kernel routines are
     written mostly in C, with some assembly language or non-standard
     C extensions covering the rest.
** Wrapup of the overview
# chapter 1 wrapup1.txt
+ Concurrency and parallelism are different.
  + Concurrency involves handling multiple tasks in the same time span.
  + Parallelism involves processing multiple tasks literally at the same time.
    + True parallelism requires multiple processors.
  + Parallelism is a performance-boosting way to manage concurrency.
+ The classic approaches to concurrency are multiprocessing and multithreading.
  + Non-blocking I/O has emerged as an option as well, but one that's
    typically used in conjunction with multiprocessing or
    multithreading.
  + Non-blocking I/O sometimes is described as /concurrency without parallelism/.
+ Different languages have difference advantages when it comes to
  studying concurrency, parallelism, and non-blocking I/O.
  + C is close to the metal: the system libraries and core OS routines at work under the hood.
  + Whatever the application language, multiprocessing,
    multithreading, and non-blocking I/O calls in the language hit the
    C libraries and, from there, kernel-space routines.
  + Java (C#), Go, Node.js and other languages have particular styles
    and strengths with respect to concurrency in particular.
* Parallelität vs. Nebenläufigkeit im Detail
** Processes and threads 
# chapter 2: processThread.txt
+ A process is a /program in execution/, each with its own address space.
  + Executing a program from the command-line is an example:
      =% hi.exe=
    + =%= is the command-line prompt, =hi.exe= is an executable program.
  + The address space comprises the memory locations accessible to the process.
  + Separate address spaces effectively partition memory among the processes.
+ A thread is a sequence of executable instructions within a process.
  + A 1-instruction thread is trivial, and threads made up of tens of
     instructions are more typical.
  + A single-threaded process has one thread, whereas a multithreaded process has multiple ones.
  + Although there are different ways to implement threads (more on this later), the
     critical point is this:
    + Threads within a process share the same address space--they have
      access to exactly the same memory locations. This is the root
      cause of race conditions.
    + A /race condition/ arises when two or more threads concurrently
      access the same memory location and at least one of the threads
      tries to update the location. For instance, each thread might be
      executing the instruction:\\
         =n = random_num()=
      + =n= is a shared memory location among the threads,
      + =random_num()= returns a random number
      + The result is unpredictable, except that the last thread to execute this „wins“.
+ Scheduling on modern systems: to schedule a process on a processor is to schedule one of its threads.
#+begin_example
  Process1    scheduled   +----------+
    Thread11------------->|processor3|
    Thread12              +----------+

  Process2
    Thread21
    Thread22  scheduled   +----------+
    Thread23------------->|processor7|
                          +----------+
#+end_example
** Multiprocessing
# chapter 2 multiprocessing.txt
A first look at multiprocessing: the command-line pipe operator =|=
+ At the command line:
  : cat file.txt | sort
  + The =cat= (short for =concatenate=) on the left is one executing
    process, and the =sort= on the right is another executing process.
  + The =sort= performs blocking I/O: it waits for all of the bytes
    from =cat= before doing the sorting.
  + If the machine has two processorss, =cat= could execute on one and =sort= on the other.
  + The unnamed pipe operator | thus performs automatic multiprocessing.
    + A pipe is thus a mechanism for inter-process communication (IPC).
+ Conceptual view of the pipe:
#+begin_example
                       pipe
               +--------|-------+
    writer---->|WriteEnd|ReadEnd|<----reader
               +--------+-------+
#+end_example
+ The tasks at hand (producing the bytes in the file and then sorting
  the lines) are divided between two processes, =cat= and =sort=.
+ Two general approaches in code:
  + A parent process „forks” (clones, spawns) a child process, and both
      execute code from the same program.	  
      + An if-else or the equivalent typically separates the parent code from the child code.
  + A parent process forks a child, which then executes a separate program.
+ A final example to underscore the point:
  : sleep 5 | echo 'Hello, world!'  ## sleep sends no bytes to the pipe, and echo expects none
  + Hello, world! appears on the screen, and roughly 5 seconds later the prompt returns from the sleep command.
** Multithreading
# chapter 2: multithreading.txt
A first look at multithreading: the Tomcat web server, written in Java

+ Tomcat implements the /one-thread-per-request/ model for handling client requests.
+ Tomcat runs as a single process, which is multithreaded.
+ Web sites and services are deployed as JAR files (with a .war
  extension) dropped into a particular directory (=TOMCAT_HOME/webapps=)
  or subdirectory thereof.
  + The WAR file's name (e.g., =site1= in =site1.war=) becomes the first part of /resource path(
      in a request URL:\\
      http://somemachine.org:8080/site1/...    ## site1 for the site1.war file\\
      http://somemachine.org:8080/site1/hi.jsp ## a completed URL, with hi.jsp as the requested resourc
  + Tomcat delegates each request for any WAR file to a thread, the /one-thread-per-request/ model
    + Code within the WAR file is thus susceptible to race conditions
      because multiple request-handling threads have access to the
      very same memory locations: the burden is on the programmer to
      ensure proper thread coordination.
+ For efficiency, Tomcat creates a thread pool at start-up: indeed, two thread pools--one for
   requests over HTTP, another for requests over HTTPS if HTTPS is enabled.
+ Other Java-centric web servers, such as Jetty, implement the one-thread-per-request model as well.
** Non-blocking I/O
# chapter 2: nonblocking.txt
A first look at non-blocking I/O non-blocking I/O: two code examples

+ A blocking =getItem= (read) example
#+BEGIN_SRC html
    <!doctype html>
    <html>
      <head>
        <title>Blocking I/O</title>
        <script>
          localStorage.name = 'Flintstone';             <!-- line 1 -->
          alert('Before blocking call');                <!-- line 2 -->
          var value = localStorage.getItem('name');     <!-- line 3 -->
          alert('After blocking call: ' + value);       <!-- line 4 -->
        </script>
      </head>
      <body/>
    </html>
#+END_SRC 
  + The second alert occurs only after the blocking call to 'getItem' (line 3) returns.

+ A non-blocking 'fetch' example in pseudo-code
#+BEGIN_SRC java
    define callbackFunction(Http-response) {
       productList = extractProductListFromResponse(Http-response)
       print(productList)                            ## print is blocking
    }

    // Main code 
    url = 'http://some-machine.org/some-script-to-get-data'
    connection = getConnectionToRemoteMachine(url)
    connection.fetch(callbackFunction)                   ## fetch(...) is non-blocking
    print('Hello, world!')                               ## print is blocking
    print('Goodbye, cruel world!')                       ## print is blocking
#+END_SRC
** Zusammenfassung 
 Three approaches to concurrency: multiprocessing, multithreading, non-blocking I/O

#+begin_example 
     client requests  +------------+   ## Task1: handle request1
   ------------------>| web server |   ## Task2: handle request2
                      +------------+      ...
#+end_example

+ Multiprocessing: dispatch each task to a separate process (/program
  in execution/)
  + For efficiency, "prefork" the processes by building a pool of these
   at start-up.  Then grab a process from the pool to act as the /task
   handler/.
  + When the task-handling process finishes its work, put it to sleep.
  + Apache2, nginx, and IIS are production-grade examples.
+ Multithreading: dispatch each task to a separate thread within a process.
  + Again for efficiency, build a thread-pool at start-up, and grab a
    thread from the pool to act as the /task handler/.
   + When the task-handling thread finishes its work, put it to sleep.
   + Tomcat and Jetty are production-grade examples.
+ Non-blocking I/O: in its pure form, a single-threaded process that
  jumps quickly from one task to another, perhaps doing only partial
  processing of each task.
  + For example, the non-blocking web server might read (and cache)
    some bytes as part of Task1, then do the same for Task2, and so on
    until all of the request bytes for a given task have been read.
   + Node.js is the obvious /pure/ example (but even Node.js takes a hybrid approach).
** Hybrid approaches
# chapter 2: hybrid.txt
+ There's no hard-and-fast rule about how best to support concurrency.
  + Multiprocessing, multithreading, and non-blocking I/O can be used
    in various combinations.
+ Sample hybrids
  + Multiprocessing + multithreading: IIS (Windows web server) and
    AspNet runtime
  #+begin_example
  client side                         server side
+-------------+  HTTP requests    +-----------------+
| web browser |------------------>| Web server (IIS)|
|             |<------------------|                 |
+-------------+  HTTP responses   +-------+---------+
                                          |
                                    aspnet_isapi.dll    ## Links IIS process with workers
                                          |
                                          +--- aspnet_wp.exe    ## worker process 1
                                          |
                                          +--- aspnet_wp.exe    ## worker process 2
                                          ...                   ...
  #+end_example
    + Each worker process is multithreaded (by default, 10 threads per
      worker process), and each thread handles a single request (the
      /one-thread-per-request/ model)
  + Multiprocessing + non-blocking I/O: nginx
    + Multiprocessing:
      + nginx has a /master process/ to read configuration files and
        to watch over worker processes (e.g., start a new worker if
        one happens to die unexpectedly).
      + A /worker process/ handles a client request: a given worker
        may handle even thousands of requests concurrently--given the
        efficiency of non-blocking I/O.
      + Other (optional) processes (e.g., a cache loader and a cache
        manager) also are in play.
      + Non-blocking I/O:
        + The workers employ non-blocking I/O in handling requests,
          each of which is partitioned into a collection of
          sub-requests. A completed sub-request generates an event,
          which is handled in the course of assembling a full response
          out of pieces.
  + Non-blocking I/O + multithreading (under the hood): Node.js
  #+begin_example
               Node.js server  ## a single-threaded process managing an event loop
client request  +--------+
--------------->|        |     ## 'workers' are JavaScript functions that read and
     ...        | event           otherwise handle a client's request using non-blocking 
client request  | loop   |        I/O and callbacks to signal task completion
--------------->|        |
                +--------+     ## Long-running tasks (e.g., accessing a DB) are delegated 
			          to under-the-hood threads, with callbacks to signal
                                  task completion
  #+end_example

* Multiprocessing -- Grundlagen
** A first multiprocessing example in C
*** Overview
# chapter 3 firstExample.txt
... key points of the /unnamed pipe example/

+ When the program pipeExample.c is compiled and then executed from
  the command-line, it executes as a process. Here's the command to
  execute:
  : % ./pipeExample   ## On Windows: % pipeExample
+ The executing pipeExample is the /parent/ process.
+ The parent process calls the library function fork() to spawn a
  /child/ process, a clone of the parent.
  + The library function fork() makes a system call to the OS routine that spawns the child.
  + The library function fork() returns one of three values:
    + -1 :: to signal failure (e.g., the system already has too
            many processes)
    + 0 ::  to the child process
    + > 0 :: to the parent process; in particular, the /process id/
             (pid) of the child is returned to the parent, and a pid
             is always a positive integer
  + The parent and child process both execute the code that follows the call to fork().
    + fork() effectively /clones/ one process (child) from another (parent).
    + An if-else or equivalent construct is used to ensure that the
      parent does one thing and the child does something else.
      #+BEGIN_SRC C
pid_t pid = fork();   /** pid_t is a signed integer data type **/
if (-1 == pid) {      /** error: couldn't clone a new process **/
    ...                
}   
else if (0 == pid) {  /** child code follows **/
    ...
}
else {                /** parent code follows **/
    ...
}
      #+END_SRC
  + The parent waits for the child to exit so that the child does not
    become a „zombie“.
    + When a child terminates, the OS notifies the parent. If the
      parent is already terminated, it cannot be notified--and the
      child becomes an „zombie“, dead but still around.
*** using the shell
#+BEGIN_SRC sh
bash-3.2$ ls pipeExample.c 
pipeExample.c
bash-3.2$ gcc -o pipeExample pipeExample.c 
bash-3.2$ ./pipeExample
This is the winter of our discontent
bash-3.2$ 
#+END_SRC
*** Source code
#+BEGIN_SRC C
/** A first multiprocessing example: pipes in C 
    To compile (Unix-like system): gcc -o pipeExample pipeExample.c
    For Windows, the GNU compiler: https://gcc.gnu.org/
**/
#include <sys/wait.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>

#define ReadEnd  0                                                  /** line 1 **/            
#define WriteEnd 1                                                

void report_and_die(const char* msg) {                              /** line 2 **/
  perror(msg);
  exit(-1);    /** failure **/
}

int main() {
  int pipeFDs[2];                                                   /** line 3 **/
  char buf;         /* 1-byte buffer */
  const char* msg = "This is the winter of our discontent\n";

  if (pipe(pipeFDs) < 0) report_and_die("pipe(...)");               /** line 4 **/

  pid_t cpid = fork();                                              /** line 5 **/
  if (cpid < 0) report_and_die("fork()");                           /** line 6 **/

  /*** child ***/
  if (0 == cpid) {                                                  /** line 7 **/
    close(pipeFDs[WriteEnd]);                                       /** line 8 **/
    while (read(pipeFDs[ReadEnd], &buf, 1) > 0)                     /** line 9 **/
      write(STDOUT_FILENO, &buf, sizeof(buf));                      /** 1ine 10 **/
    close(pipeFDs[ReadEnd]);                                        /** line 11 **/
    _exit(0); /* signal parent of immediate exit **/                /** line 12 **/
  } 
  /*** parent ***/
  else {                                                            /** line 13 **/ 
    close(pipeFDs[ReadEnd]);                                   
    write(pipeFDs[WriteEnd], msg, strlen(msg));                     /** line 14 **/
    close(pipeFDs[WriteEnd]); /* generates an EOF */                /** line 15 **/
    wait(0); /*** wait for child to exit ***/                       /** line 16 **/  
    exit(0); /* exit normally */                                    /** line 17 **/
  }
  return 0;
}
#+END_SRC
** Second Example -- Named Pipes
*** Overview
# chapter 3 secondExample.txt
 A second multiprocessing example in C: key points of the /named pipe/ example

+ Two separate processes, but no parent/child relationship between them.
  + The processes communicate through a /named pipe/ (aka /FIFO/).
    + A named pipe is a mechanism designed to support IPC among processes on the same host.
      #+begin_example
           +------------+
writer---->| named pipe |<----reader
           +------------+
      #+end_example
+ The system implements an FIFO as a temporary file.
+ Named pipes are quite similar in functionality to Unix Domain
  Sockets, also called /local sockets/.
  + Network sockets support communcation between processes running on
    different hosts, whereas named pipes and local sockets do the same
    for processes running on the same host.
+ The example consists of two executable programs, fifoWriter and
  fifoReader, which can be executed at the command-line as follows:
  : % ./fifoWriter &    ## for Windows: fifoWriter & (& puts the process in the background) 
  : % ./fifoReader      ## for Windows: fifoReader
  + There are now two processes executing: fifoWriter and fifoReader
  + The fifoWriter writes randomly generated integers to the FIFO.
  + The fifoReader reads these integers from the FIFO until end-of-file.
  + In the C code, the FIFO is /written/ and /read/ as if it were a
    regular disk file: the I/O API is the same.
*** using the shell
#+BEGIN_SRC sh
bash-3.2$ ./fifoWriter &
[1] 7316
bash-3.2$ ./fifoReader
60749779
966803328
1202260494
...
[1]+  Done                    ./fifoWriter
bash-3.2$ 
#+END_SRC
*** Source Code =fifoWriter.c=
#+BEGIN_SRC C
#include <fcntl.h> 
#include <stdio.h>
#include <time.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/stat.h>
#include <sys/types.h>

#define MaxWrites 256  

int main() {
  srand(time(0)); /* seed the random number generator */

  const char* fifoName = "./myPipe1";          
  mkfifo(fifoName, 0666); /* read/write for user/group/others */             /** line 1 **/
  int fd = open(fifoName, O_CREAT | O_WRONLY); /* open blocks on a FIFO */   /** line 2 **/

  int i;
  for (i = 0; i < MaxWrites; i++) { 
    int n = rand();             
    write(fd, &n, sizeof(int));  /* 4 bytes per int */                       /** line 3 **/
  }
  close(fd);         /* close the fifo */                                    /** line 4 **/
  unlink(fifoName);  /* dispose of underlying file */                        /** line 5 **/
  return 0;
}
#+END_SRC
*** Source Code =fifoReader.c=
#+BEGIN_SRC C
#include <fcntl.h> 
#include <stdio.h>
#include <time.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/stat.h>
#include <sys/types.h>

int main() {
    const char* fifoName = "./myPipe1";
    int num;
    int fd = open(fifoName, O_RDONLY);              /** line 1 **/

    while (read(fd, &num, sizeof(num)) > 0)         /** line 2 **/ 
      printf("%i\n", num);
    close(fd);                                      /** line 3 **/
    unlink(fifoName);                               /** line 4 **/
    return 0;
}
#+END_SRC
** The Nginx Web Server
# chapter 3: nginx.txt
 Multiprocessing in production-grade software: the nginx web server

+ www.nginx.com
  + The web server is written in C.
  + nginx combines multiprocessing and non-blocking I/O as concurrency mechanisms.

+ nginx, like any modern web server that relies on multiprocessing for concurrency,
   is a "pre-forking" server.
  + nginx can be started as a /system service/ (at boot time), and is platform-neutral.

+ =nginx.conf=  ()configuration entries are key/value pairs: no XML!)
  #+begin_example 
user www-data;           ## www-data (name is arbitrary) is owner of workers
worker_processes 4;      ## default number of workers, might be set to processor count
pid /var/run/nginx.pid;  ## file /var/run/nginx.pid stores pid of master process

events {
   worker_connections 1024;  ## Max of 1024 connections per worker 
   ...                       ## (typical browsers open at least two connections to a 
                                site per client session)
    }   
  #+end_example

+ =ps -ef | grep nginx=
  #+begin_example 
Owner     pid   ppid   nginx process type

root      1803     1   master process /usr/sbin/nginx  ## command that started nginx
www-data  1804  1803   worker process
www-data  1805  1803   worker process
www-data  1807  1803   worker process
www-data  1808  1803   worker process
  #+end_example
** Execing and Forking Options in Multiprocessing
# chapter 3 execOverview.txt
 A second way for one executing program to start another

+ Review of the previously way, introduction of the new way:
  #+begin_example
        fork()
parent---------->child        ## Both parent and child continue to execute...

         exec...()
process1----------->process2  ## process2 replaces process1, which terminates
  #+end_example
+ In systems speak, an /image/ is an executable program. For instance,
  copying the system from one machine to another is called
  /re-imaging/.
  + In the multiprocessing /pipeExample/ code, the image is the
    compiled, executable program named /pipeExample/
+ A new process can be /spawned/ (created) under one of two distinct
  scenarios, which the difference between the fork() function and the
  exec-family of functions illustrates.
  + The exec-family consists of library functions that, under the
    hood, make the same system call.
+ In a successful call to fork(), the new child process executes the
  /same/ image as the parent that spawned it: the forked process is a
  clone. So the standard code idiom is this:
  #+BEGIN_SRC C
pid_t pid = fork();  /* parent executes this */
...                  /* check for success */
if (0 == pid) {      /* 0 returned to the child */
   ...               /* child code */
}
else {               /* parent code: fork returns child's pid to parent */
   ...               /* alternative parent test: if (pid > 0)
}
  #+END_SRC
+ In a successful call to an exec-function, replaces the current process image with
   a new image that's loaded into memory.
  #+BEGIN_SRC c
int flag = execl("/usr/bin/myGame", "myGame", 0);
if (-1 == flag) perror("Couldn't exec...");
/* on success, the 'myGame' program now executes */
  #+END_SRC
  + The new image has its own address space and other features. In
    summary, the old process stops execution and a new one begins
    execution.
  + The new process retains the pid (and ppid) of the one it replaces
    + The user and group properties are likewise unchanged.
*** Using the Shell
#+BEGIN_SRC sh
bash-3.2$ ./fileStatus tmp
File name: tmp

Information about file tmp:

Owner ID:    503
Group ID:    20
Byte size:   68
Last access: Fri Oct  6 11:10:23 2017
File type:   directory
Owner read:  Owner readable
Owner write: Owner writable

bash-3.2$ ./fileStatus fileStatus
File name: fileStatus

Information about file fileStatus:

Owner ID:    503
Group ID:    20
Byte size:   8808
Last access: Fri Oct  6 11:12:05 2017
File type:   regular
Owner read:  Owner readable
Owner write: Owner writable

bash-3.2$ ./execing
File name: tmp

Information about file tmp:

Owner ID:    503
Group ID:    20
Byte size:   68
Last access: Fri Oct  6 11:10:23 2017
File type:   directory
Owner read:  Owner readable
Owner write: Owner writable
#+END_SRC
*** Source Code
#+BEGIN_SRC C
/* To run: 

   -- compile this program (gcc -o execing execing.c) and fileStatus.c 
   --                      (gcc -o fileStatus fileStatus.c)
   -- create a subdirectory named tmp: mkdir tmp
   -- from the command line: ./execing

   This program 'execs' the fileStatus program, which outputs metadata about the tmp dir..
   Here's a depiction:

                   execv
   execing program------->fileStatus program
*/

#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>

int main() {
  char* const args[] = {"./fileStatus", "tmp", 0};  /* fileStatus program, tmp directory */
  int ret = execv("./fileStatus", args);            /* "v" for "vector" */
  if (-1 == ret) {                                  /* check for failure */
    perror("execv(...) error");
    exit(-1);
  }
  else 
    printf("I'm here\n");                           /* never executes */
  return 0;
}
#+END_SRC
** Process Tracking and Management
# chapter 3 processTracking.txt
 Process tracking and context switching

+ Each process has a unique ID (pid), a non-negative integer.
  + A process's pid is recorded in the system's process table.
  + Library functions getpid() and getppid() get the pid and the
    parent's pid, respectively.
  + The pid of 0 is typically reserved for the /idle process/, the one
    that /runs/ when there's nothing else to run.
  + The /init process/ has a pid of 1: the first user-space process
    that the OS kernel spawns in the boot-up.
    + On shutdown, the init process is the last process to terminate:
      it waits for all of its children.
    + The init process starts other processes: system services such as
      the login service, etc.
+ Key data structures in a process /context/:
  + process table: tracks information per process, in particular
    information that allows a pre-empted process to restart later.
  + page table(s): translate virtual into physical addresses. Each
    process has its own, which effectively partitions virtual address
    spaces among processes.
  + file table: tracks, per process, files that a process has opened.
+ Every process has a user and group /owner/, which determines the
  process's access rights to resources.
  + Every process likewise belongs to a /process group/, which
    facilitates sending signals to every process in a group, to
    getting information about related processes, and so on.
  + In the default case, a child process belongs to the same as its
    parent.
+ Most modern systems use pre-emptive scheduling, with either
  fixed-length or variable-length time-slices: a process that exceeds
  the time-slice on a given run is pre-empted and sent back to a
  scheduling queue.
  + A process and a thread /context/ consists of key information about
    it.
    + For a process, key data structures such as the process table,
      the file table, and its page tables define the /context/.
+ Context switches, which occur during pre-emption, come in two
  flavors:
  + From one thread to another within the same process: /intraprocess
    switching/
  + From one process to another: /interprocess switching/
    + The big cost is the swapping out of one (virtual) address space
      for another: page tables
  + A process-level context switch is signficantly more expensive than
    a thread-level context switch.
    + The cost of thread-level switching is near zero on modern systems.
    + Depiction:
      #+begin_example
Process1      Process2        processor7
  Thread11      Thread21
  Thread12

1st scenario: Thread12 replaces Thread11 on processor7  + virtually no overhead

2nd scenario: Thread21 replaces Thread11 on processor7  + high overhead--
                                                          process context switch
      #+end_example
** Multiprocessing Through Richer Code Examples
# eventuell überspringen
Chapter 4 : 
+ node.js
+ ipc
+ nginx
+ exercise 
+ upsides and downsides of multiprocessing
*** Overview
* Multithreading -- Grundlagen
#   Chapter 5 : WICHTIG !!! (Java)
** Overview
# chapter 5: multithreadingOverview.txt
 Multithreading overview

+ The chief advantage of multithreading over multiprocessing is that
  threads are /lighter/ than processes:
  + A process consists of one or more threads of execution.
  + A process-level context switch is significantly more expensive, in the millisecond range.
  + A thread-level context switch has a near-zero cost for threads in
    the same process.
+ The chief disadvantage is that threads in the same process share the
  same address space:
  + The programmer, rather than the OS, must ensure that two threads
    in the same process don't inappropriately access the same memory
    location (e.g., by trying to update the location).
  + The programmer must coordinate threads in the same process to
    avoid /race conditions/.  
    + If the coordination is overdone, /deadlock/ may result.
  + Thread coordination must be just right:
    + Too little brings the threat of race conditions and, therefore,
      unpredictable results.
    + Too much hurts efficiency and may result in deadlock.
+ Some systems (e.g., Linux) implement threads as processes that
  happen to share an address space; other systems (e.g., Windows) have
  distinct kernel-level support for threads.
  + Under any implementation, the standard behavior is that distinct
    processes have distinct address spaces, whereas threads in the
    same process share an address space.
+ Examples in C, Java, and Go
  + C# and Java have very similar support for threading, although both
    sides might disagree with this characterization. :)
*** A Sample Race Condition in Java
**** source code
#+BEGIN_SRC java
/** To compile: javac RaceC.java
    To run:     java Main

    Flow of control for Threads referenced by t1 and t2 (for convenience,
    call the Threads referenced by t1 and t2 simply 't1' and 't2'):

    The 'main thread' is the thread that executes the method 'main', in
    this case in class Main (see below).

                 creates
    main thread----------->t1 and t2

                  starts
    main thread----------->t1 and t2

                 calls run()
    Java runtime------------->on started Threads t1 and t2

    At this point, three threads in the app are executing: main, t1, and t2.

    An instance of a Java Thread represents the sequence of instructions in the
    body of the encapsulated 'run' method (and any other instructions in methods
    that 'run' invokes).

    A thread that exits run() terminates, and cannot be restarted.
*/
public class RaceC {
    static int n;                            // a single instance on n
    static void race() {
	n = 0;                               // initialize to zero before threads alter its value
	long limit = Integer.MAX_VALUE * 2L; // four billion and change
	Thread t1 = new Thread() {           // incrementing thread
		public void run() { 
		    for (long i = 0; i < limit; i++) n = n + 1; // increment limit times
		}
	    };
	Thread t2 = new Thread() {           // decrementing thread
		public void run() {
		    for (long i = 0; i < limit; i++) n = n - 1; // decrement limit times
		}
	    };
	t1.start();  // start t1's execution
	t2.start();  // start t2's execution
	try {
	    t1.join();  // wait here until t1 terminates
	    t2.join();  // wait here until t2 terminates
	} catch(Exception e) { }
	System.out.println("n's value is: " + n);
    }
}

class Main {
    public static void main(String[ ] args) {         //*** main thread executes method main
	for (int i = 0; i < 8; i++) RaceC.race(); 
    }
}
/** Output from a sample run:

    n's value is: -265936
    n's value is: -7317
    n's value is: 47128
    n's value is: -219153
    n's value is: 82805
    n's value is: -7944
    n's value is: 87322
    n's value is: -2
*/
#+END_SRC
**** using the shell
#+BEGIN_SRC sh
bash-3.2$ javac RaceC.java
bash-3.2$ java Main
n's value is: -332920
n's value is: 237836
n's value is: 73816
n's value is: 61941
n's value is: 59569
n's value is: -2
n's value is: -2
n's value is: 80561
#+END_SRC

*** Analysis of the Race Condition Code
# chapter 5: raceConditionanalysis.txt
 A detailed look at how a race condition occurs

+ T1 and T2 are threads in process P: T1 and T2 have access the same
  memory locations.
+ N is memory location within the address space that T1 and T2 share
+ T1 tries to increment n at the same time that T2 tries to
  decrement N.
  #+begin_example
      N = N + 1  +---+  N = N - 1        ## Are these statements 'atomic'?
   T1----------->| 5 |<-----------T2
	         +---+
	           N
  #+end_example
+ Each pseudo-code instruction involves two operations: an addition or
  subtraction, then an assignment
  + Assume that the result of the addition/subtraction is stored in a
    CPU register or on the stack: in either case, in a temporary
    location. Here's a depiction:
    #+begin_example
 Temp1 = N + 1  ### increment N and save the incremented value (N is unchanged so far)
 N = Temp1      ### assign the incremented value to N
    #+end_example
    + Temp1 is T1's temporary location, Temp2 is T2's temporary location
+ The machine has multiple CPUs: T1 executes on one of these as T2
  executes on another
+ The following depicts one possible outcome of a race condition:
  #+begin_example
     Clock ticks:
  
   C1: Temp1 = 5 + 1 = 6   ## T1's addition
   C2: Temp2 = 5 - 1 = 4   ## T2's subtraction (T1's assignment has not occurred yet.)
   C3: N = Temp2           ## T2's assignment operation (N is decremented to 4)
   C4: N = Temp1           ## T1's assignment operation (N is incremented to 6).
  #+end_example
+ After a single increment by 1 and a single decrement by 1, N winds up as 6--not 5.
  + Improper interleaving of the operations is at fault.
  + Whichever thread starts its arithmetic operation
    (addition/subtraction) must be allow to complete the assignment
    without interruption.
    + This is precisely what /thread synchronization/ through locking
      of N ensures: single-threaded execution of the arithmetic
      operation and the subsequent assignment.
*** Overview of Explicit Thread Locking
**** code examples
#+BEGIN_SRC java
/* Thread synchronization mechanisms: code examples */

/** C example **/
static pthread_mutex_t lock;  /* declare a lock */              /** line 1 **/
void increment_n() {                                            /** line 2 **/
    pthread_mutex_lock(&lock);                                  /** line 3 **/
    n = n + 1;  /* critical section code */                     /** line 4 **/
    pthread_mutex_unlock(&lock);                                /** line 5 **/
}

/** Java example **/
class Test {
    static int n; // a single, shared storage location          /** line 6 **/
    static Object lock = new Object(); // can't be null         /** line 7 **/
    void incrementN() {
	synchronized(lock) {                                    /** line 8 **/
	    n = n + 1; /* critical section code */              /** line 9 **/
	}                                                       /** line 10 **/
    }
}
#+END_SRC
****  Mechanisms for thread coordination
# chapter 5: threadSync.txt
+ The terms below are associated with processes and threads, but the
  focus now is on threads given the assumption that a process executes
  in that one of its threads executes.
+ A /critical section/ of code must be executed in single-threaded fashion because
   otherwise a race condition could arise.
  + For example, if threads T1 and T2 can both execute 
    : N = N + 1  ## the same storage N is accessible to T1 and T2
    this statement is a critical section of code. T1 and T2 should not
    execute this section at the same time.
+ A locking mechanism should provide the following services:
  + The mechanism should ensure /mutual exclusion/: if T1 manages to
    grab the lock, then T2 is excluded from the critical section that
    the lock protects while T1 holds the lock; and vice versa.
  + The mechanism should ensure /progress/: if no thread holds the
    lock, then some thread should be able to grab it and thereby enter
    the critical section that the lock protects.
+ Various locking mechanisms:
  + A /semaphore/ restricts the number of threads allowed to access a
    shared resource (e.g., a shared function). For example, /semahore/
    might allow two threads to access a chunk of code simultaneously,
    but no more than two. A semaphore is thus a set of permission
    tickets, which enable a thread to access a resource.
    + Semaphores as /tickets/: a semaphore is like a ticket that
      grants access to a resource.  A semaphore with a value of three
      would grant access to three threads at most at a time.
  + A /mutex/ is a semaphore with a value of 1: whichever thread holds
    the mutex has access to the protected resource, whereas all others
    are excluded.
    + A mutex enforces mutual exclusion, but a semaphore with a value
      > 1 would not.
  + A /monitor/ (which the Java /synchronized/ block provides) is a
    mechanism that enforces mutual exclusion, supports progress, and
    has addition mechanisms for thread cooperation: in Java's case,
    the /wait/ mechanism supports quiet waiting for a lock to be
    released, and the /notify/ mechanism notifies waiters that a lock
    has been released.

*** A Sample Deadlock in Java
**** using the shell
#+BEGIN_SRC sh
bash-3.2$ javac Deadlock.java 
bash-3.2$ java Deadlock
thread2 holds lock2
thread1 holds lock1
thread2 waiting for lock1
	(thread2 needs lock1 to release lock2...)
thread1 waiting for lock2
	(thread1 needs lock2 to release lock1...)
  ^C ^Cbash-3.2$ 
bash-3.2$ java Deadlock
thread1 holds lock1
thread2 holds lock2
thread2 waiting for lock1
thread1 waiting for lock2
	(thread2 needs lock1 to release lock2...)
	(thread1 needs lock2 to release lock1...)
  ^C ^Cbash-3.2$ 
#+END_SRC
**** source code
#+BEGIN_SRC java
/** To compile and run from the command-line:
      javac Deadlock.java
      java Deadlock
*/
public class Deadlock {
    static Object lock1 = new Object(); // a single lock1                   /** line 1 **/
    static Object lock2 = new Object(); // a single lock2
    
    /** Each thread executes its run() method. **/
    public static void main(String args[]) {
	Thread thread1 = new Thread() {
		public void run() {                            
		    synchronized(lock1) {                                   /** line 2 **/
			print("thread1 holds lock1");
			try { Thread.sleep(2); } catch(Exception e) { }     /** line 3 **/
			print("thread1 waiting for lock2");
			print("\t(thread1 needs lock2 to release lock1...)");
			synchronized(lock2) {                               /** line 4 **/
			    print("thread1 holds lock1 and lock2");
			} // lock2 released here                            /** line 5 **/
		    } // lock1 released here                                /** line 6 **/
		} 
	    };
	Thread thread2 = new Thread() {
		public void run() {
		    synchronized(lock2) {                                   /** line 7 **/
			print("thread2 holds lock2");
			try { Thread.sleep(2); } catch(Exception e) { }
			print("thread2 waiting for lock1");
			print("\t(thread2 needs lock1 to release lock2...)");
			synchronized(lock1) {                               /** line 8 **/
			    print("thread2 holds lock2 and lock1");
			} // lock1 released here                            /** line 9 **/
		    } // lock2 released here                                /** line 10 **/
		}
	    };
	thread1.start();                                                    /** line 11 **/
	thread2.start();                                                    /** line 12 **/
    }
    private static void print(Object obj) { System.out.println(obj); }
}
#+END_SRC

*** High-level Concurrency Management in Multithreading
# chapter 5: highlevelcm.txt
+ Belaboring the obvious: multithreading is far trickier to manage than multiprocessing
  + The OS keeps the processes in multiprocessing from stepping on one another's storge.
  + The programmer must do the same for threads within a process: thread coordination
+ How to support to thread-safety (i.e., safety from race conditions)?
  + Consider an in-memory mutable list and an /add/ operation.
  #+begin_example
       ADD 3  +---+---+
thread------->| 1 | 2 |        ## list L before the ADD
              +---+---+
              +---+---+---+
              | 1 | 2 | 3 |    ## list L after the ADD, which changes the original list
              +---+---+---+
  #+end_example
+ Basic Java approach: Ensure mutual exclusion on shared storage through locking.
  + Locking allows only single-threaded access to the list L during an ADD operation.

+ Basic Clojure approach: make in-memory objects immutable by default, with a few exceptions
  + The ADD operation first makes a copy of the original list, and then changes the copy.
  #+begin_example
       ADD 3  +---+---+
thread------->| 1 | 2 |        ## list L 
              +---+---+
                  L

              +---+---+
              | 1 | 2 |        ## copy of list L
              +---+---+
                  Lc

              +---+---+---+
              | 1 | 2 | 3 |    ## copy after the ADD (original is unchanged)
              +---+---+---+
                  Lc
  #+end_example
+ The preferred Go approach: have a single thread control access to
  list L, with other threads sending messages such as ADD to this
  controlling thread.
  + The messages are sent through a thread-safe channel.
  #+begin_example
             +---+---+
             | 1 | 2 |      ## a single thread T controls access to L
             +---+---+

        ADD 3                ADD 33
thread1------->channel to T<--------thread2  ## the channel has built-in synchronization
  #+end_example
*** Wrapup of multithreading basics and miscellany
# chapter 5: wrapup5.txt
**** Wrapup

+ Multithreading can be a highly efficient way to do concurrent
  programming, as thread-level context switches are very cheap.
  + It's the preferred approach to concurrency in languages such as
    Java and C#.
+ Multithreading has twin challenges for the programmer:
  + Avoid race conditions by ensuring that multiple threads don't
    access shared memory locations in inappropriate ways.
    + Ensure that critical sections are executed in single-threaded
      mode.
  + Avoid /over synchronization/ that degrades performance and may
    result in deadlock.
+ Languages now offer programmers high-level constructs and types to
  ease the challenges of multithreaded programming.
  + But sometimes a simple mutex is still the way to go.
 
**** Native versus green threads, and the Global Interpreter Lock (GIL)

+ A /native thread/ is under kernel OS control when it comes to
  scheduling.
  + Since roughly 2000, Java Thread instances map to /native/
    threads. (Same for C#.)
  + C's /pthreads/ (the 'p' for POSIX API standardization) are
    /native/ threads.
+ A /green thread/ (aka /microthread/, /tasklet/) is under a particular language's runtime control.
  + In effect, /green threads/ emulate native threads, and may provide
    better performance for operations such as thread creation and
    synchronization.
  + Prior to version 1.9, for example, Ruby's standard implementation (CRuby) had only green threads.
+ A GIL is a mechanism (in implementation, a mutex) that a runtime
  uses to allow only one thread to execute at a time, even in a
  multithreaded environment: no thread-level parallelism.
  + The standard implementations of Ruby (CRuby) and Python (CPython) have a GIL.

*** Exercise for basic multithreading
# chapter 5: exerciseMT.txt

+ Fix the RaceC program (RaceC.java) by providing explicit locking.
  + Use synchronized blocks and a shared lock to ensure that threads
    t1 and t2 do the increment and decrement operations /atomically/
    (that is, without interruption).
  + *Hint:* use a static field as the lock to ensure there's a single instance of the lock.
+ To get a sense of what thread syncrhonization costs, time the RaceC
  program with and without thread synchronization.
  + The program's execution can be timed in various ways, including the way sketched below:
  #+BEGIN_SRC java
final long startTime = System.currentTimeMillis();
/*** code to be timed ***/
final long stopTime = System.currentTimeMillis();
final long latency = stopTime - startTime;
  #+END_SRC

** Multithreading Through Richer Code Examples 
# ev. überspringen / Studi-Thema
Chapter 6
** Thread-Safety And High-Level Concurrent Types In Java
Chapter 7 
# ev. Studi-Thema
** Options For Thread Synchronization And Cooperation
Chapter 8
# ???
** Concurrency And Non-Blocking I-O
Chapter 9
# ev. überspringen

* Parallele Algorithmen
# Titel fraglich
** Parallelism Basics
# Chapter 10
*** Overview of Parallelism Beyond Concurrency
# Chapter 10 parallelismOverview.txt
 Parallelism in computing

+ So far the emphasis has been on parallelism as concurrency made
  efficient: multiple concurrent tasks are distributed among
  processors so that the tasks can be processed literally at the same
  time--in parallel.
+ In a multiprocessing approach to concurrency, multiple processes
  execute on multiple processors.
  + The OS handles the scheduling.
+ In a multithreading approach to concurrency, multiple threads within a process execute on multiple processors.
  + If API threads map to /native/ threads, then the OS handles the scheduling. (Assuming no GIL.)
+ In hybrid approaches (e.g., Asp.Net's multithreaded worker
  processes), there's still the distribution of task processing among
  multiple processors.

The goal now is to focus on broader uses of parallel computation.

+ Some reasons why parallelism is now so attractive:
  + Consumer desktop, laptop, and even handheld machines are now
    multi-core, with one or more processors per core.
    + In fact (even if not in reputation), these machines are /parallel computers/.
    + At issue is how to exploit this possibily for parallel computations.
  + Consumer-level machines may come with GPUs (graphics-processing
    units) and FPUs (floating-point unit or /math coproccessor/) as
    well as CPUs (central or /generic/ processing units).
    + GPUs and FPUs are designed for parallelism.
    + /gpgpu/ (general-purpose computing on a GPU) refers to the use
      of a GPU for /general purposes/, that is, purposes beyond just
      graphics. So gpgpu approaches a GPU as if it were a
      (specialized) CPU.
  + Distributed software systems such as email and the WWW have code
    modules that execute on physically distinct devices.
    + The /network as the computer/ describes networked hosts, each
      with its own collection of processors, as a resource for
      distributed parallel computation.
    + The key challenge is how to coordinate such computations and to share information among them.
      + Message-passing is a popular, general method.
+ Parallel computations may used shared, centralized memory;
  distributed memory; or some hybrid of the two.
  + The point of interest is how to write programs that execute as
    parallel computations.
  + Support from toolchains (in particular, compilers therein) is
    critical to encouarge widespread use of hardware support for
    parallel computing.

*** A SIMD Auto Vectorization Code Example in C
**** the non parallel version
#+BEGIN_SRC C
/** To compile and run:

    gcc -o nonsimd nonsimd.c
    ./nonsimd  # On Windows: simd
,*/
  
#include <stdio.h>

#define Length 8

int main() {
  int array1[ ] = {1, 2, 3, 4, 5, 6, 7, 8}; 
  int array2[ ] = {8, 7, 6, 5, 4, 3, 2, 1};
  int sum[Length];

  int i;
  for (i = 0; i < Length; i++) sum[i] = array1[i] + array2[i]; /** serial computation **/
  
  for (i = 0; i < Length; i++) printf("%i ", sum[i]); 
  putchar('\n');
  return 0;
}
/** A better way: SSE = Streaming SIMD Extensions = additional instruction set, 
    more CPU registers

In the late 1990s, Intel introduced an SSE instruction set for their Pentium III
processors. This became known as the MMX instruction set. (Officially, MMX is not an
acronym.) AMD earlier introduced a comparable instruction set known as 3DNow!. In any case, 
the hardware support included new registers with names that start with 'xmm'. In the 
original SSE, there was support only for integer operations.

SEE has progressed, with new versions: SSE2, SSE3, SSSE3, and
SSE4. The newer versions bring SIMD support for both integer and
floating-point operations. At the same time, C/C++ compilers such as
GNU and Clang provided support, eventually automatic, for
SIMD-friendly code. Nowadays, both compilers are capable of 'automatic
vectorization' of approrpiately written source code.

The short version: elements in the arrays 'array1' and 'array2' should
be added pairwise in parallel in order to boost performance.
,*/
#+END_SRC
**** the parallel version
#+BEGIN_SRC C
/** To compile and run:

    gcc -o simd simd.c
    ./simd  # On Windows: simd

 No explicit optimization is required. The GNU and Clang compilers
 do the optimization automatically. 
*/
  
#include <stdio.h>

/* A typedef in C supports 'aliasing' of data types. Here's a simple example:

     typedef unsigned boolean; ## 'unsigned' is short for 'unsigned int'

   This typedef allows 'boolean' to be used in place of 'unsigned', for example:

     boolean flag = 0;  ## 0 is 'false' in C

   The typedef below is more complicated, as it includes special information for the
   compiler, in particuar the '__attribute__' and 'vector_size' terms. Here's the upshot
   of the typedef:

   # 'intV8' aliases 'int', but as a vector (1-dimensional array) rather than as a scalar 
      type.
   # The size of an 'intV8' vector is set to 32 == Length * sizeof(int) == 8 * 4. 
     The size is in bytes.
*/
#define Length 8
typedef int intV8 __attribute__ ((vector_size (Length * sizeof(int)))); /** 32 bytes **/
    
int main() {
  intV8 dataV1 = {1, 2, 3, 4, 5, 6, 7, 8}; /* initialize vector dataV1: no array syntax */                   
  intV8 dataV2 = {8, 7, 6, 5, 4, 3, 2, 1}; /* same for vector dataV2 */
  
  intV8 add = dataV1 + dataV2; /* 9  9  9  9  9  9  9 9 -- no array syntax */
  intV8 mul = dataV1 * dataV2; /* 8 14 18 20 20 18 14 8 -- no array syntax */
  intV8 div = dataV2 / dataV1; /* 8  3  2  1  0  0  0 0 -- no array syntax */
  int i;
  for (i = 0; i < Length; i++) printf("%i ", add[i]); /* array syntax */
  putchar('\n');
  return 0;
}
#+END_SRC
*** using the shell
#+BEGIN_SRC sh 
bash-3.2$ gcc -o nonsimd nonsimd.c
bash-3.2$ gcc -o simd simd.c

bash-3.2$ ./nonsimd
9 9 9 9 9 9 9 9 

bash-3.2$ ./simd
9 9 9 9 9 9 9 9 
#+END_SRC
*** in Assembly-Code
# chapter 10 simdDoc.s
# ev. auslassen
 Some compiler-generated assembly code from the simd.c program
#+BEGIN_SRC
# A note on C compilation:

                 +------------+    +--------+    +---------+    +------+
C source code--->|preprocessor|--->|compiler|--->|assembler|--->|linker|--->executable 	
                 +------------+    +--------+ /  +---------+ /  +------+    machine code
                                             /              /
                                        assembly code    object code

## Compile command: gcc -S simd.c  (produces 'simd.s' by default)	
	
main:
   ## The identifiers that start with $ are constants.
	
   ## %rsp is the /stack pointer/, which points to the TOP position on the stack.
	
   ## An expression such as 96(%rsp) is an address or pointer expression: %rsp is the base 
      address, 96 is of the offset therefrom. The offset is in bytes, so in this case it/s 
      96 bytes. (The Intel stack grows downwards, from high to low addresses.)

   ## The full expression 'movl $1, 96(%rsp)' copies (not literally moves) the 32-bit value 
      1 into stack location %rsp + 96:

	The stack
	  ...                     ## high addresses
	+-----+
	|  1  |<------%rsp + 96   ## instruction 'movl $l, 96(%rsp)' puts 1 in this location
        +-----+
	  ...                
	+-----+
	|     |
	+-----+
	|     |<------%rsp   ## %rsp is the 64-bit 'stack pointer'
	+-----+
	  ...                     ## low addresses
	
   ## Note that every offset increases by 4 because the values (1,2,3,...) are 4-byte 
      (32-bit) integer values.
   movl $1, 96(%rsp)         # data vector 1 start
   movl $2, 100(%rsp)
   movl $3, 104(%rsp)
   movl $4, 108(%rsp)
   movl $5, 112(%rsp)
   movl $6, 116(%rsp)
   movl $7, 120(%rsp)
   movl $8, 124(%rsp)        # data vector 1 end
   movl $8, 128(%rsp)        # data vector 2 start
   movl $7, 132(%rsp)
   movl $6, 136(%rsp)
   movl $5, 140(%rsp)
   movl $4, 144(%rsp)
   movl $3, 148(%rsp)
   movl $2, 152(%rsp)
   movl $1, 156(%rsp)        # data vector 2 end
	
   movdqa 96(%rsp), %xmm1    # %xmm1 = dataV1[0..3]: 'dq' is for 'double quadword'
	                     # (quad == 64 bits) and %xmm1 is a 128-bit register
	
   movdqa 128(%rsp), %xmm0   # %xmm0 = dataV2[0..3]
	
   paddd %xmm0, %xmm1        # %xmm1 += %xmm0 (the 'paddd' instruction is for 'packed 
                               addition' of 32-bit values)
   movdqa 112(%rsp), %xmm2   # %xmm2 = dataV1[4..7]
   movdqa 144(%rsp), %xmm0   # %xmm0 = dataV2[4..7]
   paddd %xmm2, %xmm0        # %xmm0 += %xmm2
	
   movdqa %xmm1, 32(%rsp)    # save %xmm1 to stack (sums of the 1st four elements)
   movdqa %xmm0, 48(%rsp)    # save %xmm0 to stack (sums of the last four elements)
	                     # the sum is now on the stack contiguously, in effect as an array
   ...

#+END_SRC
** Time the Performance Boost From Parallelism -- a Code Example
# chapter 10
*** Source Code
# ssETimings.c
#+BEGIN_SRC C
/** A timing example to contrast serial and parallel execution on typical matrix/vector 
    operations. The program uses 'SSE intrinsics', the interface for which is the header 
    file 'xmmintrin.h'.
    
    Some of the code is obscure, a good sign of what's involved as you get ever closer 
    to the metal.

    The program adds two vectors, and then multiples the rows of the sum vector by a 
    +third vector.
**/

#include <stdio.h>
#include <xmmintrin.h>
#include <stdint.h>

typedef void (*func)();  /** a function that takes no arguments and returns void **/

#define Length      (2048)
#define AlignedVals    (4)

 /** Ensure 16-byte alignment of vector elements: 16 bytes is 4 ints or 4 floats,
     which comes to 128 bits--the size of an %xmm register. **/
float v1[Length] __attribute__ ((aligned(16))); 
float v2[Length] __attribute__ ((aligned(16)));
float v3[Length] __attribute__ ((aligned(16)));
float r1[Length] __attribute__ ((aligned(16)));
float r2[Length] __attribute__ ((aligned(16)));

/** Initialize with small random values. **/
void init() {
  unsigned i;
  for (i = 0; i < Length; i++) {
    v1[i] = (float) (rand() / RAND_MAX); 
    v2[i] = (float) (rand() / RAND_MAX);  
    v3[i] = (float) (rand() / RAND_MAX); 
  }
}

void in_serial() {
  unsigned i;
  for (i = 0; i < Length; i++) r1[i] = (v1[i] + v2[i]) * v3[i]; /** the math **/
}

void in_parallel() {
  /** set up for the special vector instructions **/
  __m128* mm_v1 = (__m128*) v1; 
  __m128* mm_v2 = (__m128*) v2;
  __m128* mm_v3 = (__m128*) v3;
  __m128* mm_rv = (__m128*) r2;

  /** Multiply a vector row by a vector: the outer instruction is _mm_mul_ps. 
      _mm_add_ps adds vector elements.
   **/
  unsigned i;
  unsigned n = Length / AlignedVals; /** in this case, n == 512 **/
  for( i = 0; i < n; i++) mm_rv[i] = _mm_mul_ps(_mm_add_ps(mm_v1[i], mm_v2[i]), mm_v3[i]);
}

/** from the Intel developer's guide: in-line assembly for reasonably good timings **/
uint64_t rdtsc() {
    uint32_t lo, hi;
    __asm__ __volatile__ (
      "xorl %%eax, %%eax\n"
      "cpuid\n"
      "rdtsc\n"
      : "=a" (lo), "=d" (hi)
      :
      : "%ebx", "%ecx");
    return (uint64_t) hi << 32 | lo;
}

void time_it(const char* msg, func func2run) {
  unsigned long long start, stop;
  start = rdtsc();
  func2run();
  stop = rdtsc();
  printf("%s: %lld\n", msg, stop - start);
}

int main() {
  init();
  time_it("in_serial()  ", in_serial);
  time_it("in_parallel()", in_parallel);
  return 0;
}
#+END_SRC
*** using the shell
#+BEGIN_SRC sh
bash-3.2$ gcc -o sseTimings sseTimings.c
bash-3.2$ ./sseTimings
in_serial()  : 19905
in_parallel(): 10483
bash-3.2$ ./sseTimings
in_serial()  : 22260
in_parallel(): 11319
bash-3.2$ ./sseTimings
in_serial()  : 21785
in_parallel(): 10957
#+END_SRC
** OpenMp
# noch Chapter 10
# entweder nachtragen oder an Studi vergeben
*** Overview
*** Basics
*** Miser-Spendthrift Problem
** Parallelism and Performance
# chapter 11
*** How Can Performance Gains Through Parallelism Be Measured
 Parallelism and performance

+ Revised /sseTimings/ program, now in 3 versions: /timings2048/,
  /timings8192/, and /timings32768/:
  + Average speedup of 'serial' version from 20 test runs:\\
       'timings2048':  average speedup of 2.17    ## 2048 is vector size\\
       'timings8192':  average speedup of 1.72    ##  8192 == 2048 x 4\\
       'timings32768': average speedup of 1.65    ## 32768 == 8192 x 4
  + Key question: The 'in_parallel()' function does only 1/4th the
    arithmetic operations as the 'in_serial()' function. Why is max
    speedup above only slightly over 2 rather than close to (or even
    at) 4?
------------------------------------------------------------------------------------------------------------   
+ How to measure performance?
  + Response time ('latency'): elapsed time
    + Program P1 takes 10 units, whereas program P2 takes 5: P1/P2 =
      2, a speedup of 2
  + Throughput: number of tasks completed in a fixed amount of time.
    + On single-processor machine M1, with required times for processing above the arrows:
      #+begin_example
    5       3   +-----------+
T2----->T1----->| processor |  ## Two tasks completed in 8 time units on M1
                +-----------+
      #+end_example
    + On multiple-processor machine M2:
      #+begin_example
   5   +-----------+
T1----->| processor | ## T1 takes a bit longer on M2
       +-----------+
               ## Two tasks completed in 5 time units on M2, a speedup of about 1.6
   5   +-----------+
T2----->| processor | ## T2 takes the same amount of time on M1 and M2
       +-----------+
      #+end_example
  + Big picture: parallelism is a good way to boost performance
    + Parallelism tends to increase throughput, and thereby performance.

*** The ParallelSort Program in Java
#+BEGIN_SRC java
/** A Java program to illustrate the potential speedup of parallelism, in this
    case for sorting: the program uses the new (as of Java 8) Arrays.parallelSort
    method, and also the Arrays.parallelSetAll method for parallel initialization
    of the roughly 4M array of integers.

    As noted, Java 8 or greater is requied.
*/
import java.util.Random;
import java.util.Arrays;

public class ParallelSort {
    private int[ ] array2Sort;
    
    public static void main(String[ ] args) {
	new ParallelSort().doIt();
    }
    private void doIt() {
	final int arraySize = 1024 * 1024 * 4;  // about 4M: 4,194,304     /** line 1 **/

	//** serial sort
        init(arraySize);
	long serial = sortArray(array2Sort, false);                        /** line 2 **/

	//** parallel sort
	init(arraySize);
	long parallel = sortArray(array2Sort, true);                       /** line 3 **/

	//** report
	log("Serial sort of array of size:   " + arraySize);
	log("Elapsed time: " + serial);
	log("");

	log("Parallel sort of array of size: " + arraySize); 
	log("Elapsed time: " + parallel);

	log("");
	double ratio = (double) serial / (double) parallel;
	if (parallel < serial) 
	    log("Speedup:  " + ratio);
	else
	    log("Slowdown: " + ratio);
    }
    private void init(int size) {
	array2Sort = new int[size];
	/** parallel initialization of array elements **/
	Arrays.parallelSetAll(array2Sort,                                  /** line 4 **/ 
			      i->new Random().nextInt());                  /** line 5 **/
    }
    private long sortArray(int[ ] array, boolean parallel) {
	long start = System.currentTimeMillis();
	/** parallel version of merge-sort **/
	if (parallel) Arrays.parallelSort(array);                          /** line 6 **/
	/** variant of quicksort ('dual pivot') **/
	else Arrays.sort(array);                                           /** line 7 **/
	long stop = System.currentTimeMillis();
	return stop - start;
    }
    private void log(String msg) {
	System.out.println(msg);
    }
}
#+END_SRC
**** using the shell
#+BEGIN_SRC sh
bash-3.2$ java ParallelSort
Serial sort of array of size:   4194304
Elapsed time: 824

Parallel sort of array of size: 4194304
Elapsed time: 358

Speedup:  2.3016759776536313
bash-3.2$ java ParallelSort
Serial sort of array of size:   4194304
Elapsed time: 710

Parallel sort of array of size: 4194304
Elapsed time: 258

Speedup:  2.751937984496124
bash-3.2$ java ParallelSort
Serial sort of array of size:   4194304
Elapsed time: 807

Parallel sort of array of size: 4194304
Elapsed time: 295

Speedup:  2.7355932203389832
#+END_SRC
*** Amdahls Law by Example
 A quick look at Amdahl's Law

+ The law, named after its creator (a one-time IBM engineer who
  founded a IBM rival company), sets a theoretical limit on the
  speedup that can be achieved in a computation.
  + The law has the practical benefit of sharpening intuitions about
    how speedup should be pursued.
+ A sample problem for Amdahl's law: 
  + Compute the arithmetic mean of 4M floating-point values on
    single-processor machine M1.  Call this problem P.
  + Compute the speedup of porting the solution to quad-processor
    machine M4.
    + Assume that M1 and M4 differ only in the number of processors.
+ A step-by-step application of the law:
  1. Compute response time for P on M1: $RT1$
  2. Compute response time for P on M4: $RT4$
  3. Computer the ratio: $RT1/RT4$
     + If $\frac{RT1}{RT4} = 1$, the response times are the same: no difference between M1 and M4 performance.
     + If $\frac{RT1}{RT4} < 1$, there's slowdown rather than speedup.
     + If $\frac{RT1}{RT4} > 1$, there's speedup.
  4. Normalize $RT1$ to 1 time unit: keeps things simple.
  5. Break $RT4$ into two parts: 
     + $enchanced$: in this case, parallel and
     + $unenhanced$: in this case, serial
     + The $unenhanced$ part of $RT4$ is the part still done in serial
       mode -- no benefit of parallelism.
       + The data set needs to be partitioned, assigned to workers
         (threads or processes), the results from the workers need to
         be aggregated, etc.
     + The $enhanced$ part of $RT4$ is the part that benefits from the
       speedup due to parallelism.
     + $$Speedup = \frac{RT1}{RT4} = \frac{1}{unenhanced + enhanced}$$
     + The $enhanced$ depends on two factors: what fraction of time is
       sped up, and by what amount.  How much time does M4 speed in
       'parallel mode' when solving P, and how much faster is M4 in
       'parallel mode' than M1 in serial mode?
     + Assume M4 is four times faster than M1 when M4 is in 'parallel
       mode', and that M4 is in 'parallel mode' 80% of the time when
       solving P.
     + $$Speedup = \frac{RT1}{RT4} = \frac{1}{unenhanced + enhanced} = \frac{1}{0.2 + \frac{0.8}{4.0}} = 2.5$$
     + Under the given assumptions, M4 solves P 2.5 times faster than does M1.
+ The big picture of Amdahal's Law:
  + It's tempting to be distracted by how much faster a 'local
    speedup' is: M4 is four times faster than M1 when M4 is in
    'parallel mode'.
  + It's critical to focus on the fraction of time spent in enhanced
    mode: M4 is in 'parallel mode' 80% of the time--not 100% of time.

| Machine | Fraction in local speedup | Local speedup | Global speedup |
|---------+---------------------------+---------------+----------------|
| M4      |                       0.8 |           4.0 |            2.5 |
| M8      |                       0.7 |           8.0 |            2.6 |
| M12     |                       0.6 |          12.0 |          *2.2* |
| M16     |                       0.5 |          16.0 |            1.9 |
| M20     |                       0.4 |          20.0 |            1.6 |
| M24     |                       0.3 |          24.0 |            1.4 |


*** Programming exercise for parallelism/performance
+ More data gathering and reflecting on the data than coding
+ The current version calls Arrays.sort(...) and Arrays.parallelSort(...) on an array.
  + Array has roughly 4M array elements.
  + Speedup on the machines I tried was about 2.5 for the parallelSort.
+ In the interest of gathering more data, change the program so that
  it tries various sizes.  For instance:
  #+BEGIN_SRC java
  final int initialSize = 1024;
  final int howMany = 16; // don't go over 20 -- overflow occurs
  int[ ] arraySizes = new int[howMany];
  arraySizes[0] = initialSize;
  for (int i = 1; i < howMany; i++) {
      arraySizes[i] = arraySizes[i - 1] * 2;
  }
  #+END_SRC
  Get the speedup for each of the sizes, and note any trends in speedup.
+ Do the same for numeric types other than int, for instance, long, float, and double.
** Parallelism And Distributed Computing
# chapter 12
+ Map/Reduce in Java
+ OpenMPI
* weitere verwendete Literatur
+ cite:CACM2017
+ cite:CACM2016
+ cite:Subramanian2017
* bibliography:referenzen.bib
 
